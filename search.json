[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\n\n\n\n\n\n\n\n\n\n\n\n\nIt is already doing it\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nRandom-Walk Bayesian Deep Networks: Dealing with Non-Stationary Data\n\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nThomas Wiecki & Ravin Kumar\n\n\n\n\n\n\n\n\nUsing Bayesian Decision Making to Optimize Supply Chains\n\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2019\n\n\nThomas Wiecki & Ravin Kumar\n\n\n\n\n\n\n\n\nUsing Bayesian Decision Making to Optimize Supply Chains\n\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2019\n\n\nThomas Wiecki & Ravin Kumar\n\n\n\n\n\n\n\n\nAn intuitive, visual guide to copulas\n\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2018\n\n\nThomas Wiecki\n\n\n\n\n\n\n\n\nAn intuitive, visual guide to copulas\n\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2018\n\n\nThomas Wiecki\n\n\n\n\n\n\n\n\nMCMC sampling for dummies\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2015\n\n\nThomas Wiecki\n\n\n\n\n\n\n\n\nMCMC sampling for dummies\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2015\n\n\nThomas Wiecki\n\n\n\n\n\n\n\n\nThe Best Of Both Worlds: Hierarchical Linear Regression in PyMC3\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2014\n\n\nThomas Wiecki & Danne Elbers\n\n\n\n\n\n\n\n\nThe Best Of Both Worlds: Hierarchical Linear Regression in PyMC3\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2014\n\n\nThomas Wiecki & Danne Elbers\n\n\n\n\n\n\n\n\nDummy - New in pymc 3.1\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2014\n\n\nThomas Wiecki & Danne Elbers\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/GLM_hierarchical.html",
    "href": "blog/GLM_hierarchical.html",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "Thomas Wiecki & Danne Elbers 2020\n\nThe power of Bayesian modelling really clicked for me when I was first introduced to hierarchical modelling. In this blog post we will highlight the advantage of using hierarchical Bayesian modelling as opposed to non-hierarchical Bayesian modelling. This hierachical modelling is especially advantageous when multi-level data is used, making the most of all information available by its ‘shrinkage-effect’, which will be explained below.\nHaving multiple sets of measurements comes up all the time, in Psychology for example you test multiple subjects on the same task. You then might want to estimate a model that describes the behavior as a set of parameters relating to mental functioning. Often we are interested in individual differences of these parameters but also assume that subjects share similarities (being human and all). Software from our lab, HDDM, allows hierarchical Bayesian estimation of a widely used decision making model but we will use a more classical example of hierarchical linear regression here to predict radon levels in houses.\nThis is the 3rd blog post on the topic of Bayesian modeling in PyMC3, see here for the previous two:\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\nThis world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n\n\n\nGelman et al.’s (2007) radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all county’s of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to enter the house through the basement. Moreover, its concentration is thought to differ regionally due to different types of soil.\nHere we’ll investigate this difference and try to make predictions of radon levels in different countys and where in the house radon was measured. In this example we’ll look at Minnesota, a state that contains 85 county’s in which different measurements are taken, ranging from 2 till 80 measurements per county.\n\n\n\nradon\n\n\nFirst, we’ll load the data:\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm \nimport pandas as pd\n\ndata = pd.read_csv('radon.csv')\n\ncounty_names = data.county.unique()\ncounty_idx = data['county_code'].values\n\nThe relevant part of the data we will model looks as follows:\n\ndata[['county', 'log_radon', 'floor']].head()\n\n\n\n\n\n\n\n\ncounty\nlog_radon\nfloor\n\n\n\n\n0\nAITKIN\n0.832909\n1.0\n\n\n1\nAITKIN\n0.832909\n0.0\n\n\n2\nAITKIN\n1.098612\n0.0\n\n\n3\nAITKIN\n0.095310\n0.0\n\n\n4\nANOKA\n1.163151\n0.0\n\n\n\n\n\n\n\nAs you can see, we have multiple radon measurements (log-converted to be on the real line) in a county and whether the measurement has been taken in the basement (floor == 0) or on the first floor (floor == 1). Here we want to test the prediction that radon concentrations are higher in the basement.\n\n\n\n\n\nNow you might say: “That’s easy! I’ll just pool all my data and estimate one big regression to asses the influence of measurement across all counties”. In math-speak that model would be:\n\\[radon_{i, c} = \\alpha + \\beta*\\text{floor}_{i, c} + \\epsilon\\]\nWhere \\(i\\) represents the measurement, \\(c\\) the county and floor contains which floor the measurement was made. If you need a refresher on Linear Regressions in PyMC3, check out my previous blog post. Critically, we are only estimating one intercept and one slope for all measurements over all counties.\n\n\n\nBut what if we are interested whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then you might say “OK then, I’ll just estimate \\(n\\) (number of counties) different regresseions – one for each county”. In math-speak that model would be:\n\\[radon_{i, c} = \\alpha_{c} + \\beta_{c}*\\text{floor}_{i, c} + \\epsilon_c\\]\nNote that we added the subindex \\(c\\) so we are estimating \\(n\\) different \\(\\alpha\\)s and \\(\\beta\\)s – one for each county.\nThis is the extreme opposite model, where above we assumed all counties are exactly the same, here we are saying that they share no similarities whatsoever which ultimately is also unsatisifying.\n\n\n\nFortunately there is a middle ground to both of these extreme views. Specifically, we may assume that while \\(\\alpha\\)s and \\(\\beta\\)s are different for each county, the coefficients all come from a common group distribution:\n\\[\\alpha_{c} \\sim \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)\\] \\[\\beta_{c} \\sim \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta}^2)\\]\nWe thus assume the intercepts \\(\\alpha\\) and slopes \\(\\beta\\) to come from a normal distribution centered around their respective group mean \\(\\mu\\) with a certain standard deviation \\(\\sigma^2\\), the values (or rather posteriors) of which we also estimate. That’s why this is called multilevel or hierarchical modeling.\nHow do we estimate such a complex model with all these parameters you might ask? Well, that’s the beauty of Probabilistic Programming – we just formulate the model we want and press our Inference Button(TM).\nNote that the above is not a complete Bayesian model specification as we haven’t defined priors or hyperpriors (i.e. priors for the group distribution, \\(\\mu\\) and \\(\\sigma\\)). These will be used in the model implementation below but only distract here.\n\n\n\n\n\n\nTo really highlight the effect of the hierarchical linear regression we’ll first estimate the non-hierarchical Bayesian model from above (separate regressions). For each county a new estimate of the parameters is initiated. As we have no prior information on what the intercept or regressions could be we are placing a Normal distribution centered around 0 with a wide standard-deviation. We’ll assume the measurements are normally distributed with noise \\(\\epsilon\\) on which we place a Half-Cauchy distribution.\n\n# takes about 45 minutes\nindiv_traces = {}\nfor county_name in county_names:\n    # Select subset of data belonging to county\n    c_data = data.loc[data.county == county_name]\n    c_data = c_data.reset_index(drop=True)\n    \n    c_log_radon = c_data.log_radon\n    c_floor_measure = c_data.floor.values\n    \n    with pm.Model() as individual_model:\n        # Intercept prior\n        a = pm.Normal('alpha', mu=0, sigma=1)\n        # Slope prior\n        b = pm.Normal('beta', mu=0, sigma=1)\n    \n        # Model error prior\n        eps = pm.HalfCauchy('eps', beta=1)\n    \n        # Linear model\n        radon_est = a + b * c_floor_measure\n    \n        # Data likelihood\n        y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=c_log_radon)\n\n        # Inference button (TM)!\n        trace = pm.sample(progressbar=False)\n        \n    indiv_traces[county_name] = trace\n\n\n\n\n\nInstead of initiating the parameters separatly, the hierarchical model initiates group parameters that consider the county’s not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county’s \\(\\alpha\\) and \\(\\beta\\).\n\nwith pm.Model() as hierarchical_model:\n    # Hyperpriors\n    mu_a = pm.Normal('mu_alpha', mu=0., sigma=1)\n    sigma_a = pm.HalfCauchy('sigma_alpha', beta=1)\n    mu_b = pm.Normal('mu_beta', mu=0., sigma=1)\n    sigma_b = pm.HalfCauchy('sigma_beta', beta=1)\n    \n    # Intercept for each county, distributed around group mean mu_a\n    a = pm.Normal('alpha', mu=mu_a, sigma=sigma_a, shape=len(data.county.unique()))\n    # Intercept for each county, distributed around group mean mu_a\n    b = pm.Normal('beta', mu=mu_b, sigma=sigma_b, shape=len(data.county.unique()))\n    \n    # Model error\n    eps = pm.HalfCauchy('eps', beta=1)\n    \n    # Expected value\n    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n    \n    # Data likelihood\n    y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=data.log_radon)\n\n\nwith hierarchical_model:\n    hierarchical_trace = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [eps, beta, alpha, sigma_beta, mu_beta, sigma_alpha, mu_alpha]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 29 seconds.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 47 divergences after tuning. Increase `target_accept` or reparameterize.\nThe acceptance probability does not match the target. It is 0.7013813577935659, but should be close to 0.8. Try to increase the number of tuning steps.\nThe rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\nThe estimated number of effective samples is smaller than 200 for some parameters.\n\n\n\n    \n        \n      \n      100.00% [4000/4000 00:18&lt;00:00 Sampling 2 chains, 50 divergences]\n    \n    \n\n\n\npm.traceplot(hierarchical_trace);\n\n\n\n\nThe marginal posteriors in the left column are highly informative. mu_a tells us the group mean (log) radon levels. mu_b tells us that the slope is significantly negative (no mass above zero), meaning that radon concentrations are higher in the basement than first floor. We can also see by looking at the marginals for a that there is quite some differences in radon levels between counties; the different widths are related to how much measurements we have per county, the more, the higher our confidence in that parameter estimate.\n\nAfter writing this blog post I found out that the chains here (which look worse after I just re-ran them) are not properly converged, you can see that best for sigma_beta but also the warnings about “diverging samples” (which are also new in PyMC3). If you want to learn more about the problem and its solution, see my more recent blog post “Why hierarchical models are awesome, tricky, and Bayesian”.\n\n\n\n\n\n\nTo find out which of the models works better we can calculate the Root Mean Square Deviaton (RMSD). This posterior predictive check revolves around recreating the data based on the parameters found at different moments in the chain. The recreated or predicted values are subsequently compared to the real data points, the model that predicts data points closer to the original data is considered the better one. Thus, the lower the RMSD the better.\nWhen computing the RMSD (code not shown) we get the following result:\n\nindividual/non-hierarchical model: 0.13\nhierarchical model: 0.08\n\nAs can be seen above the hierarchical model performs a lot better than the non-hierarchical model in predicting the radon values. Following this, we’ll plot some examples of county’s showing the true radon values, the hierarchial predictions and the non-hierarchical predictions.\n\nselection = ['CASS', 'CROW WING', 'FREEBORN']\nfig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\naxis = axis.ravel()\nfor i, c in enumerate(selection):\n    c_data = data.loc[data.county == c]\n    c_data = c_data.reset_index(drop = True)\n    z = list(c_data['county_code'])[0]\n\n    xvals = np.linspace(-0.2, 1.2)\n    for a_val, b_val in zip(indiv_traces[c]['alpha'][::10], indiv_traces[c]['beta'][::10]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.05)\n    axis[i].plot(xvals, indiv_traces[c]['alpha'][::10].mean() + indiv_traces[c]['beta'][::10].mean() * xvals, \n                 'b', alpha=1, lw=2., label='individual')\n    for a_val, b_val in zip(hierarchical_trace['alpha'][::10][z], hierarchical_trace['beta'][::10][z]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.05)\n    axis[i].plot(xvals, hierarchical_trace['alpha'][::10][z].mean() + hierarchical_trace['beta'][::10][z].mean() * xvals, \n                 'g', alpha=1, lw=2., label='hierarchical')\n    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n                    alpha=1, color='k', marker='.', s=80, label='original data')\n    axis[i].set_xticks([0,1])\n    axis[i].set_xticklabels(['basement', 'first floor'])\n    axis[i].set_ylim(-1, 4)\n    axis[i].set_title(c)\n    if not i%3:\n        axis[i].legend()\n        axis[i].set_ylabel('log radon level')\n\n\n\n\nIn the above plot we have the data points in black of three selected counties. The thick lines represent the mean estimate of the regression line of the individual (blue) and hierarchical model (in green). The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are.\nWhen looking at the county ‘CASS’ we see that the non-hierarchical estimation has huge uncertainty about the radon levels of first floor measurements – that’s because we don’t have any measurements in this county. The hierarchical model, however, is able to apply what it learned about the relationship between floor and radon-levels from other counties to CASS and make sensible predictions even in the absence of measurements.\nWe can also see how the hierarchical model produces more robust estimates in ‘CROW WING’ and ‘FREEBORN’. In this regime of few data points the non-hierarchical model reacts more strongly to individual data points because that’s all it has to go on.\nHaving the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa."
  },
  {
    "objectID": "blog/GLM_hierarchical.html#the-data-set",
    "href": "blog/GLM_hierarchical.html#the-data-set",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "Gelman et al.’s (2007) radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all county’s of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to enter the house through the basement. Moreover, its concentration is thought to differ regionally due to different types of soil.\nHere we’ll investigate this difference and try to make predictions of radon levels in different countys and where in the house radon was measured. In this example we’ll look at Minnesota, a state that contains 85 county’s in which different measurements are taken, ranging from 2 till 80 measurements per county.\n\n\n\nradon\n\n\nFirst, we’ll load the data:\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm \nimport pandas as pd\n\ndata = pd.read_csv('radon.csv')\n\ncounty_names = data.county.unique()\ncounty_idx = data['county_code'].values\n\nThe relevant part of the data we will model looks as follows:\n\ndata[['county', 'log_radon', 'floor']].head()\n\n\n\n\n\n\n\n\ncounty\nlog_radon\nfloor\n\n\n\n\n0\nAITKIN\n0.832909\n1.0\n\n\n1\nAITKIN\n0.832909\n0.0\n\n\n2\nAITKIN\n1.098612\n0.0\n\n\n3\nAITKIN\n0.095310\n0.0\n\n\n4\nANOKA\n1.163151\n0.0\n\n\n\n\n\n\n\nAs you can see, we have multiple radon measurements (log-converted to be on the real line) in a county and whether the measurement has been taken in the basement (floor == 0) or on the first floor (floor == 1). Here we want to test the prediction that radon concentrations are higher in the basement."
  },
  {
    "objectID": "blog/GLM_hierarchical.html#the-models",
    "href": "blog/GLM_hierarchical.html#the-models",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "Now you might say: “That’s easy! I’ll just pool all my data and estimate one big regression to asses the influence of measurement across all counties”. In math-speak that model would be:\n\\[radon_{i, c} = \\alpha + \\beta*\\text{floor}_{i, c} + \\epsilon\\]\nWhere \\(i\\) represents the measurement, \\(c\\) the county and floor contains which floor the measurement was made. If you need a refresher on Linear Regressions in PyMC3, check out my previous blog post. Critically, we are only estimating one intercept and one slope for all measurements over all counties.\n\n\n\nBut what if we are interested whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then you might say “OK then, I’ll just estimate \\(n\\) (number of counties) different regresseions – one for each county”. In math-speak that model would be:\n\\[radon_{i, c} = \\alpha_{c} + \\beta_{c}*\\text{floor}_{i, c} + \\epsilon_c\\]\nNote that we added the subindex \\(c\\) so we are estimating \\(n\\) different \\(\\alpha\\)s and \\(\\beta\\)s – one for each county.\nThis is the extreme opposite model, where above we assumed all counties are exactly the same, here we are saying that they share no similarities whatsoever which ultimately is also unsatisifying.\n\n\n\nFortunately there is a middle ground to both of these extreme views. Specifically, we may assume that while \\(\\alpha\\)s and \\(\\beta\\)s are different for each county, the coefficients all come from a common group distribution:\n\\[\\alpha_{c} \\sim \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)\\] \\[\\beta_{c} \\sim \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta}^2)\\]\nWe thus assume the intercepts \\(\\alpha\\) and slopes \\(\\beta\\) to come from a normal distribution centered around their respective group mean \\(\\mu\\) with a certain standard deviation \\(\\sigma^2\\), the values (or rather posteriors) of which we also estimate. That’s why this is called multilevel or hierarchical modeling.\nHow do we estimate such a complex model with all these parameters you might ask? Well, that’s the beauty of Probabilistic Programming – we just formulate the model we want and press our Inference Button(TM).\nNote that the above is not a complete Bayesian model specification as we haven’t defined priors or hyperpriors (i.e. priors for the group distribution, \\(\\mu\\) and \\(\\sigma\\)). These will be used in the model implementation below but only distract here."
  },
  {
    "objectID": "blog/GLM_hierarchical.html#probabilistic-programming",
    "href": "blog/GLM_hierarchical.html#probabilistic-programming",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "To really highlight the effect of the hierarchical linear regression we’ll first estimate the non-hierarchical Bayesian model from above (separate regressions). For each county a new estimate of the parameters is initiated. As we have no prior information on what the intercept or regressions could be we are placing a Normal distribution centered around 0 with a wide standard-deviation. We’ll assume the measurements are normally distributed with noise \\(\\epsilon\\) on which we place a Half-Cauchy distribution.\n\n# takes about 45 minutes\nindiv_traces = {}\nfor county_name in county_names:\n    # Select subset of data belonging to county\n    c_data = data.loc[data.county == county_name]\n    c_data = c_data.reset_index(drop=True)\n    \n    c_log_radon = c_data.log_radon\n    c_floor_measure = c_data.floor.values\n    \n    with pm.Model() as individual_model:\n        # Intercept prior\n        a = pm.Normal('alpha', mu=0, sigma=1)\n        # Slope prior\n        b = pm.Normal('beta', mu=0, sigma=1)\n    \n        # Model error prior\n        eps = pm.HalfCauchy('eps', beta=1)\n    \n        # Linear model\n        radon_est = a + b * c_floor_measure\n    \n        # Data likelihood\n        y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=c_log_radon)\n\n        # Inference button (TM)!\n        trace = pm.sample(progressbar=False)\n        \n    indiv_traces[county_name] = trace"
  },
  {
    "objectID": "blog/GLM_hierarchical.html#hierarchical-model",
    "href": "blog/GLM_hierarchical.html#hierarchical-model",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "Instead of initiating the parameters separatly, the hierarchical model initiates group parameters that consider the county’s not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county’s \\(\\alpha\\) and \\(\\beta\\).\n\nwith pm.Model() as hierarchical_model:\n    # Hyperpriors\n    mu_a = pm.Normal('mu_alpha', mu=0., sigma=1)\n    sigma_a = pm.HalfCauchy('sigma_alpha', beta=1)\n    mu_b = pm.Normal('mu_beta', mu=0., sigma=1)\n    sigma_b = pm.HalfCauchy('sigma_beta', beta=1)\n    \n    # Intercept for each county, distributed around group mean mu_a\n    a = pm.Normal('alpha', mu=mu_a, sigma=sigma_a, shape=len(data.county.unique()))\n    # Intercept for each county, distributed around group mean mu_a\n    b = pm.Normal('beta', mu=mu_b, sigma=sigma_b, shape=len(data.county.unique()))\n    \n    # Model error\n    eps = pm.HalfCauchy('eps', beta=1)\n    \n    # Expected value\n    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n    \n    # Data likelihood\n    y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=data.log_radon)\n\n\nwith hierarchical_model:\n    hierarchical_trace = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [eps, beta, alpha, sigma_beta, mu_beta, sigma_alpha, mu_alpha]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 29 seconds.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 47 divergences after tuning. Increase `target_accept` or reparameterize.\nThe acceptance probability does not match the target. It is 0.7013813577935659, but should be close to 0.8. Try to increase the number of tuning steps.\nThe rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\nThe estimated number of effective samples is smaller than 200 for some parameters.\n\n\n\n    \n        \n      \n      100.00% [4000/4000 00:18&lt;00:00 Sampling 2 chains, 50 divergences]\n    \n    \n\n\n\npm.traceplot(hierarchical_trace);\n\n\n\n\nThe marginal posteriors in the left column are highly informative. mu_a tells us the group mean (log) radon levels. mu_b tells us that the slope is significantly negative (no mass above zero), meaning that radon concentrations are higher in the basement than first floor. We can also see by looking at the marginals for a that there is quite some differences in radon levels between counties; the different widths are related to how much measurements we have per county, the more, the higher our confidence in that parameter estimate.\n\nAfter writing this blog post I found out that the chains here (which look worse after I just re-ran them) are not properly converged, you can see that best for sigma_beta but also the warnings about “diverging samples” (which are also new in PyMC3). If you want to learn more about the problem and its solution, see my more recent blog post “Why hierarchical models are awesome, tricky, and Bayesian”."
  },
  {
    "objectID": "blog/GLM_hierarchical.html#posterior-predictive-check",
    "href": "blog/GLM_hierarchical.html#posterior-predictive-check",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "To find out which of the models works better we can calculate the Root Mean Square Deviaton (RMSD). This posterior predictive check revolves around recreating the data based on the parameters found at different moments in the chain. The recreated or predicted values are subsequently compared to the real data points, the model that predicts data points closer to the original data is considered the better one. Thus, the lower the RMSD the better.\nWhen computing the RMSD (code not shown) we get the following result:\n\nindividual/non-hierarchical model: 0.13\nhierarchical model: 0.08\n\nAs can be seen above the hierarchical model performs a lot better than the non-hierarchical model in predicting the radon values. Following this, we’ll plot some examples of county’s showing the true radon values, the hierarchial predictions and the non-hierarchical predictions.\n\nselection = ['CASS', 'CROW WING', 'FREEBORN']\nfig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\naxis = axis.ravel()\nfor i, c in enumerate(selection):\n    c_data = data.loc[data.county == c]\n    c_data = c_data.reset_index(drop = True)\n    z = list(c_data['county_code'])[0]\n\n    xvals = np.linspace(-0.2, 1.2)\n    for a_val, b_val in zip(indiv_traces[c]['alpha'][::10], indiv_traces[c]['beta'][::10]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.05)\n    axis[i].plot(xvals, indiv_traces[c]['alpha'][::10].mean() + indiv_traces[c]['beta'][::10].mean() * xvals, \n                 'b', alpha=1, lw=2., label='individual')\n    for a_val, b_val in zip(hierarchical_trace['alpha'][::10][z], hierarchical_trace['beta'][::10][z]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.05)\n    axis[i].plot(xvals, hierarchical_trace['alpha'][::10][z].mean() + hierarchical_trace['beta'][::10][z].mean() * xvals, \n                 'g', alpha=1, lw=2., label='hierarchical')\n    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n                    alpha=1, color='k', marker='.', s=80, label='original data')\n    axis[i].set_xticks([0,1])\n    axis[i].set_xticklabels(['basement', 'first floor'])\n    axis[i].set_ylim(-1, 4)\n    axis[i].set_title(c)\n    if not i%3:\n        axis[i].legend()\n        axis[i].set_ylabel('log radon level')\n\n\n\n\nIn the above plot we have the data points in black of three selected counties. The thick lines represent the mean estimate of the regression line of the individual (blue) and hierarchical model (in green). The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are.\nWhen looking at the county ‘CASS’ we see that the non-hierarchical estimation has huge uncertainty about the radon levels of first floor measurements – that’s because we don’t have any measurements in this county. The hierarchical model, however, is able to apply what it learned about the relationship between floor and radon-levels from other counties to CASS and make sensible predictions even in the absence of measurements.\nWe can also see how the hierarchical model produces more robust estimates in ‘CROW WING’ and ‘FREEBORN’. In this regime of few data points the non-hierarchical model reacts more strongly to individual data points because that’s all it has to go on.\nHaving the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa."
  },
  {
    "objectID": "blog/GLM_hierarchical.html#references",
    "href": "blog/GLM_hierarchical.html#references",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "References",
    "text": "References\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\nThis world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n\nChris Fonnesbeck repo containing a more extensive analysis\nShrinkage in multi-level hierarchical models by John Kruschke\nGelman, A.; Carlin; Stern; and Rubin, D., 2007, “Replication data for: Bayesian Data Analysis, Second Edition”,\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press.\nGelman, A. (2006). Multilevel (Hierarchical) modeling: what it can and cannot do. Technometrics, 48(3), 432–435."
  },
  {
    "objectID": "blog/copulas.html",
    "href": "blog/copulas.html",
    "title": "An intuitive, visual guide to copulas",
    "section": "",
    "text": "test People seemed to enjoy my intuitive and visual explanation of Markov chain Monte Carlo so I thought it would be fun to do another one, this time focused on copulas.\nIf you ask a statistician what a copula is they might say “a copula is a multivariate distribution \\(C(U_1, U_2, ...., U_n)\\) such that marginalizing gives \\(U_i \\sim \\operatorname{\\sf Uniform}(0, 1)\\)”. OK… wait, what? I personally really dislike these math-only explanations that make many concepts appear way more difficult to understand than they actually are and copulas are a great example of that. The name alone always seemed pretty daunting to me. However, they are actually quite simple so we’re going to try and demistify them a bit. At the end, we will see what role copulas played in the 2007-2008 Financial Crisis."
  },
  {
    "objectID": "blog/copulas.html#example-problem-case",
    "href": "blog/copulas.html#example-problem-case",
    "title": "An intuitive, visual guide to copulas",
    "section": "Example problem case",
    "text": "Example problem case\nLet’s start with an example problem case. Say we measure two variables that are non-normally distributed and correlated. For example, we look at various rivers and for every river we look at the maximum level of that river over a certain time-period. In addition, we also count how many months each river caused flooding. For the probability distribution of the maximum level of the river we can look to Extreme Value Theory which tells us that maximums are Gumbel distributed. How many times flooding occured will be modeled according to a Beta distribution which just tells us the probability of flooding to occur as a function of how many times flooding vs non-flooding occured.\nIt’s pretty reasonable to assume that the maximum level and number of floodings is going to be correlated. However, here we run into a problem: how should we model that probability distribution? Above we only specified the distributions for the individual variables, irrespective of the other one (i.e. the marginals). In reality we are dealing with a joint distribution of both of these together.\nCopulas to the rescue."
  },
  {
    "objectID": "blog/copulas.html#what-are-copulas-in-english",
    "href": "blog/copulas.html#what-are-copulas-in-english",
    "title": "An intuitive, visual guide to copulas",
    "section": "What are copulas in English?",
    "text": "What are copulas in English?\nCopulas allow us to decompose a joint probability distribution into their marginals (which by definition have no correlation) and a function which couples (hence the name) them together and thus allows us to specify the correlation seperately. The copula is that coupling function.\nBefore we dive into them, we must first learn how we can transform arbitrary random variables to uniform and back. All we will need is the excellent scipy.stats module and seaborn for plotting.\n\n%matplotlib inline\n\nimport seaborn as sns\nfrom scipy import stats"
  },
  {
    "objectID": "blog/copulas.html#transforming-random-variables",
    "href": "blog/copulas.html#transforming-random-variables",
    "title": "An intuitive, visual guide to copulas",
    "section": "Transforming random variables",
    "text": "Transforming random variables\nLet’s start by sampling uniformly distributed values between 0 and 1:\n\nx = stats.uniform(0, 1).rvs(10000)\nsns.distplot(x, kde=False, norm_hist=True);\n\n\n\n\nNext, we want to transform these samples so that instead of uniform they are now normally distributed. The transform that does this is the inverse of the cumulative density function (CDF) of the normal distribution (which we can get in scipy.stats with ppf):\n\nnorm = stats.distributions.norm()\nx_trans = norm.ppf(x)\nsns.distplot(x_trans);\n\n\n\n\nIf we plot both of them together we can get an intuition for what the inverse CDF looks like and how it works:\n\nh = sns.jointplot(x, x_trans, stat_func=None)\nh.set_axis_labels('original', 'transformed', fontsize=16);\n\n\n\n\nAs you can see, the inverse CDF stretches the outer regions of the uniform to yield a normal.\nWe can do this for arbitrary (univariate) probability distributions, like the Beta:\n\nbeta = stats.distributions.beta(a=10, b=3)\nx_trans = beta.ppf(x)\nh = sns.jointplot(x, x_trans, stat_func=None)\nh.set_axis_labels('orignal', 'transformed', fontsize=16);\n\n\n\n\nOr a Gumbel:\n\ngumbel = stats.distributions.gumbel_l()\nx_trans = gumbel.ppf(x)\nh = sns.jointplot(x, x_trans, stat_func=None)\nh.set_axis_labels('original', 'transformed', fontsize=16);\n\n\n\n\nIn order to do the opposite transformation from an arbitrary distribution to the uniform(0, 1) we just apply the inverse of the inverse CDF – the CDF:\n\nx_trans_trans = gumbel.cdf(x_trans)\nh = sns.jointplot(x_trans, x_trans_trans, stat_func=None)\nh.set_axis_labels('original', 'transformed', fontsize=16);\n\n\n\n\nOK, so we know how to transform from any distribution to uniform and back. In math-speak this is called the probability integral transform."
  },
  {
    "objectID": "blog/copulas.html#adding-correlation-with-gaussian-copulas",
    "href": "blog/copulas.html#adding-correlation-with-gaussian-copulas",
    "title": "An intuitive, visual guide to copulas",
    "section": "Adding correlation with Gaussian copulas",
    "text": "Adding correlation with Gaussian copulas\nHow does this help us with our problem of creating a custom joint probability distribution? We’re actually almost done already. We know how to convert anything uniformly distributed to an arbitrary probability distribution. So that means we need to generate uniformly distributed data with the correlations we want. How do we do that? We simulate from a multivariate Gaussian with the specific correlation structure, transform so that the marginals are uniform, and then transform the uniform marginals to whatever we like.\nCreate samples from a correlated multivariate normal:\n\nmvnorm = stats.multivariate_normal(mean=[0, 0], cov=[[1., 0.5], \n                                                     [0.5, 1.]])\n# Generate random samples from multivariate normal with correlation .5\nx = mvnorm.rvs(100000)\n\n\nh = sns.jointplot(x[:, 0], x[:, 1], kind='kde', stat_func=None);\nh.set_axis_labels('X1', 'X2', fontsize=16);\n\n\n\n\nNow use what we learned above to “uniformify” the marignals:\n\nnorm = stats.norm()\nx_unif = norm.cdf(x)\nh = sns.jointplot(x_unif[:, 0], x_unif[:, 1], kind='hex', stat_func=None)\nh.set_axis_labels('Y1', 'Y2', fontsize=16);\n\n\n\n\nThis joint plot above is usually how copulas are visualized.\nNow we just transform the marginals again to what we want (Gumbel and Beta):\n\nm1 = stats.gumbel_l()\nm2 = stats.beta(a=10, b=2)\n\nx1_trans = m1.ppf(x_unif[:, 0])\nx2_trans = m2.ppf(x_unif[:, 1])\n\nh = sns.jointplot(x1_trans, x2_trans, kind='kde', xlim=(-6, 2), ylim=(.6, 1.0), stat_func=None);\nh.set_axis_labels('Maximum river level', 'Probablity of flooding', fontsize=16);\n\n\n\n\nContrast that with the joint distribution without correlations:\n\nx1 = m1.rvs(10000)\nx2 = m2.rvs(10000)\n\nh = sns.jointplot(x1, x2, kind='kde', xlim=(-6, 2), ylim=(.6, 1.0), stat_func=None);\nh.set_axis_labels('Maximum river level', 'Probablity of flooding',  fontsize=16);\n\n\n\n\nSo there we go, by using the uniform distribution as our lingua franca we can easily induce correlations and flexibly construct complex probability distributions. This all directly extends to higher dimensional distributions as well."
  },
  {
    "objectID": "blog/copulas.html#more-complex-correlation-structures-and-the-financial-crisis",
    "href": "blog/copulas.html#more-complex-correlation-structures-and-the-financial-crisis",
    "title": "An intuitive, visual guide to copulas",
    "section": "More complex correlation structures and the Financial Crisis",
    "text": "More complex correlation structures and the Financial Crisis\nAbove we used a multivariate normal which gave rise to the Gaussian copula. However, we can use other, more complex copulas as well. For example, we might want to assume the correlation is non-symmetric which is useful in quant finance where correlations become very strong during market crashes and returns are very negative.\nIn fact, Gaussian copulas are said to have played a key role in the 2007-2008 Financial Crisis as tail-correlations were severely underestimated. If you’ve seen The Big Short, the default rates of individual mortgages (among other things) inside CDOs (see this scene from the movie as a refresher) are correlated – if one mortgage fails, the likelihood of another failing is increased. In the early 2000s, the banks only knew how to model the marginals of the default rates. This infamous paper by Li then suggested to use copulas to model the correlations between those marginals. Rating agencies relied on this model heavily, severly underestimating risk and giving false ratings. The rest, as they say, is history.\nRead this paper for an excellent description of Gaussian copulas and the Financial Crisis which argues that different copula choices would not have made a difference but instead the assumed correlation was way too low."
  },
  {
    "objectID": "blog/copulas.html#getting-back-to-the-math",
    "href": "blog/copulas.html#getting-back-to-the-math",
    "title": "An intuitive, visual guide to copulas",
    "section": "Getting back to the math",
    "text": "Getting back to the math\nMaybe now the statement “a copula is a multivariate distribution \\(C(U_1, U_2, ...., U_n)\\) such that marginalizing gives \\(U_i \\sim \\operatorname{\\sf Uniform}(0, 1)\\)” makes a bit more sense. It really is just a function with that property of uniform marginals. It’s really only useful though combined with another transform to get the marginals we want.\nWe can also better understand the mathematical description of the Gaussian copula (taken from Wikipedia):\n\nFor a given \\(R\\in[-1, 1]^{d\\times d}\\), the Gaussian copula with parameter matrix R can be written as \\(C_R^{\\text{Gauss}}(u) = \\Phi_R\\left(\\Phi^{-1}(u_1),\\dots, \\Phi^{-1}(u_d) \\right)\\) where \\(\\Phi^{-1}\\) is the inverse cumulative distribution function of a standard normal and \\(\\Phi_R\\) is the joint cumulative distribution function of a multivariate normal distribution with mean vector zero and covariance matrix equal to the correlation matrix R.\n\nJust note that in the code above we went the opposite way to create samples from that distribution. The Gaussian copula as expressed here takes uniform(0, 1) inputs, transforms them to be Gaussian, then applies the correlation and transforms them back to uniform."
  },
  {
    "objectID": "blog/copulas.html#support-me-on-patreon",
    "href": "blog/copulas.html#support-me-on-patreon",
    "title": "An intuitive, visual guide to copulas",
    "section": "Support me on Patreon",
    "text": "Support me on Patreon\nFinally, if you enjoyed this blog post, consider supporting me on Patreon which allows me to devote more time to writing new blog posts."
  },
  {
    "objectID": "blog/copulas.html#more-reading",
    "href": "blog/copulas.html#more-reading",
    "title": "An intuitive, visual guide to copulas",
    "section": "More reading",
    "text": "More reading\nThis post is intentionally light on math. You can find that elsewhere and will hopefully be less confused as you have a strong mental model to integrate things into. I found these links helpful:\n\nTensorflow Probability Bijection tutorial using copulas\nWikipedia article on copulas\nMatlab tutorial\nThe underlying NB of this post\n\nWe also haven’t addressed how we would actually fit a copula model. I leave that, as well as the PyMC3 implementation, as an exercise to the motivated reader ;)."
  },
  {
    "objectID": "blog/copulas.html#acknowledgements",
    "href": "blog/copulas.html#acknowledgements",
    "title": "An intuitive, visual guide to copulas",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Adrian Seyboldt, Jon Sedar, Colin Carroll, and Osvaldo Martin for comments on an earlier draft. Special thanks to Jonathan Ng for being a Patreon supporter."
  },
  {
    "objectID": "blog/supply_chain.html",
    "href": "blog/supply_chain.html",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "",
    "text": "2019 Thomas Wiecki & Ravin Kumar\nOne big problem I often observe with data science is that it often falls short of having a true impact on the bottom line of the business. The reasons for this are manifold but I firmly believe that Bayesian modeling solves many of them. Two highlight just two:\nHowever, there still is a gap when it comes to having these outputs (posterior distributions) actually have an impact. You might be quite pleased with yourself when you give your manager not just a point-estimate for how much budget they should allocate but instead a posterior distribution, but when they make their budget allocation decision, they still need to input just a single number.\nThe solution to this problem, and one we often use for client projects at PyMC Labs, is to also model the decision making process and incorporate the model estimate directly into that. In brief, by defining a loss function that maps business decisions to outcomes we can use an optimizer to find the best decision(s) not only under the most likely scenario, but under all plausible scenarios. This not only moves Bayesian modeling from something that informs a decision to something that makes a decision, it also allows you - the modeler - to communicate your results in the only language business cares about:\nIn this blog post we want to demonstrate this powerful method with a the general problem of supply chain optimization, an area where Bayesian statistics can have a big impact.\n%matplotlib inline\n\nimport pymc3 as pm\nimport theano.tensor as tt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport arviz as az\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')"
  },
  {
    "objectID": "blog/supply_chain.html#supply-chain-optimization-to-operate-a-spaceport",
    "href": "blog/supply_chain.html#supply-chain-optimization-to-operate-a-spaceport",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Supply chain optimization to operate a spaceport",
    "text": "Supply chain optimization to operate a spaceport\nIt is the year 12119 (under our new Human Era calendar), humanity has become a space-faring civilization. Bayesians and Frequentists still argue about which statistics are better. You are a Data Scientist at a spaceport called PyMC-X (why did you think we have a rocket for our logo?) that sends humans to the moon, Mars, Europa, and the ISS 5.0 in Saturn’s orbit. You are tasked with managing the supply chain to keep the rockets running.\nWhile the rockets themselves are reusable, we need a new rocket engine for every launch. There are three suppliers we can order engines from. These suppliers have different prices, different yields (i.e. how many of their shipped engines are functional), and different maximum order amounts. While we know the prices and order sizes, the true yield we do not know and can only estimate from past orders. However, as we will use simulated data here, we will include the unobservable parameters SUPPLIER_YIELD and SUPPLIER_YIELD_SD. In reality we would not have access to this information.\n\nSUPPLIER_YIELD = np.array([.9, .5, .8]) # unknown\nSUPPLIER_YIELD_SD = np.array([.1, .2, .2]) # unknown\nPRICES = [220.0, 100.0, 120.0] # known\nMAX_ORDER_SIZE = [100, 80, 100] # known\n\nThe yield represents the percentage of engines that pass our stress tests (a faulty engine lead to the abort of the Space Shuttle launch STS-93 so this is an actual problem). Due to different manufacturing techniques, the yield varies quite a bit by supplier, which is also reflected in the price. As don’t know the true yield, we will have to estimate it from previous batches we ordered. In our example, we have ordered more times from certain suppliers than others. For example, as supplier 2 (third list item) only opened up recently, we only ordered twice from there:\n\nN_OBS = [30, 20, 2]\n\n\nnp.random.seed(100)\ndata = []\nfor supplier_yield, supplier_yield_sd, n_obs in zip(SUPPLIER_YIELD, SUPPLIER_YIELD_SD, N_OBS):\n    data.append(pm.Beta.dist(mu=supplier_yield, sd=supplier_yield_sd, shape=n_obs).random())\n    \ndata\n\n[array([0.978235  , 0.98946102, 0.99035051, 0.83762708, 0.66130327,\n        0.98785994, 0.85327018, 0.8500779 , 0.99913878, 0.89881072,\n        0.8175994 , 0.95181804, 0.91545214, 0.87137954, 0.96166603,\n        0.99033823, 0.96319861, 0.94124979, 0.96555922, 0.96606356,\n        0.92723444, 0.97736913, 0.86764773, 0.81749131, 0.98597604,\n        0.97980665, 0.77295709, 0.9584931 , 0.88875261, 0.99585613]),\n array([0.51788973, 0.67831661, 0.64888304, 0.61595363, 0.08634205,\n        0.72543455, 0.51883833, 0.5454235 , 0.30357696, 0.21743938,\n        0.54628383, 0.68559965, 0.28827533, 0.79246239, 0.65810975,\n        0.69059483, 0.59297579, 0.85482231, 0.38115298, 0.8296909 ]),\n array([0.89241857, 0.9000698 ])]\n\n\n\ndata_df = pd.DataFrame(data).T\ndata_tidy = data_df.unstack().to_frame('yield')\ndata_tidy.index = data_tidy.index.set_names(['supplier', 'obs'])\ng = sns.FacetGrid(data=data_tidy.reset_index().dropna(), col='supplier')\ng.map(sns.distplot, 'yield', kde=False);\n\n/Users/twiecki/miniconda3/envs/pymc3theano/lib/python3.8/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\nSo this is the data we have available to try and estimate the true yield of every supplier."
  },
  {
    "objectID": "blog/supply_chain.html#quick-note-on-the-generality-of-this-problem",
    "href": "blog/supply_chain.html#quick-note-on-the-generality-of-this-problem",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Quick note on the generality of this problem",
    "text": "Quick note on the generality of this problem\nWe mainly choose the spaceport example in a scifi setting because it’s fun, but the underlying problem is very general and the solution widely applicable. Almost every retailer (like Amazon) has this problem of deciding how much to order given yield and holding cost. A similar problem also occurs in insurance where you need to sell contracts which have some risk of becoming claims. Or in online advertising where you need to decide how much to bid on clicks given a budget. Even if you don’t work on these industries, the cost of any inefficiencies gets passed onto you, the customer! We also had a similar problem at Quantopian when deciding which algorithms to select in our fund and how much capital to deploy to each one. OK, back to optimizing the supply chain at PyMC-X!"
  },
  {
    "objectID": "blog/supply_chain.html#the-dynamics-of-operating-a-spaceport",
    "href": "blog/supply_chain.html#the-dynamics-of-operating-a-spaceport",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "The dynamics of operating a spaceport",
    "text": "The dynamics of operating a spaceport\nIn order to assess how many engines we need we first need to know how many rocket launches we can sell. If we buy too few we are leaving money on the table, if we buy too many we will have to put them in storage which costs money (HOLDING_COST). Let’s assume we can sell a rocket for 500 bitcoins (BTC) and it costs us 100 BTC in holding cost.\n\nSALES_PRICE = 500 \nHOLDING_COST = 100\n\nNext, let’s define our loss function which takes as inputs how many engines we have in stock, how many launches customers want, at what price we bought the engine, at what price we can sell the launch, and what the holding costs are per engine:\n\n@np.vectorize\ndef loss(in_stock, demand, buy_price, sales_price=SALES_PRICE, holding_cost=HOLDING_COST):\n    # How much do we earn per launch\n    margin = sales_price - buy_price\n    # Do we have more in stock than demanded?\n    if in_stock &gt; demand:\n        total_profit = demand * margin\n        # everything left over after demand was met goes into holding\n        total_holding_cost = (in_stock - demand) * holding_cost\n        reward = total_profit - total_holding_cost\n    else:\n        # Can only sell what we have in stock, no storage required\n        reward = in_stock * margin\n    \n    # Usually we minimize, so invert\n    return -reward\n\n\nin_stock = np.arange(0, 100)\nplt.scatter(in_stock, -loss(in_stock, 50, 50)); plt.axvline(50, c='k', ls='--', label='assumed demand');\nplt.xlabel('in stock'); plt.ylabel('profit (neg loss)'); sns.despine(); plt.legend();\n\n\n\n\nAs you can see, if customer demand is 50 launches, we maximize our profit if we have 50 engines in stock. Having fewer engines eats into our profits at a greater rate than ordering excess engines because in this setup our margins are larger than the holding cost.\nNext, we need our estimate of demand. As we have a long history of launches we have a pretty good idea of what the distribution looks like, but we will also assume that we don’t know the true underlying parameters and only have access to the samples:\n\ndemand_samples = stats.poisson(60, 40).rvs(1000)\nsns.distplot(demand_samples);\n\n\n\n\nWe can evaluate our objective function over every demand we observed historically (setting engines in stock to 100):\n\nplt.scatter(demand_samples, -loss(in_stock=100, demand=demand_samples, buy_price=10))\nplt.xlabel('demand'); plt.ylabel('profit (neg loss)'); plt.axvline(100, c='k', ls='--', label='assumed in stock');\nplt.legend();\n\n\n\n\nIn response to demand, the loss-function behaves differently: with less demand than what we have in stock, we earn less (because we sell fewer launches but also have to pay holding costs), but as demand exceeds the number of engines we have in stock our profit stays flat because we can’t sell more than what we have."
  },
  {
    "objectID": "blog/supply_chain.html#estimating-yield-with-a-bayesian-model",
    "href": "blog/supply_chain.html#estimating-yield-with-a-bayesian-model",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Estimating yield with a Bayesian model",
    "text": "Estimating yield with a Bayesian model\nLet’s use PyMC3 to build a model to estimate the yield of every engine supplier:\n\nwith pm.Model() as model:\n    # Priors on alpha and beta parameters for each supplier\n    α = pm.HalfNormal('α', sd=10., shape=3) + 1\n    β = pm.HalfNormal('β', sd=10., shape=3) + 1\n    \n    # Different likelihood for every supplier because we have different\n    # number of data points\n    for i, d in enumerate(data):\n        pm.Beta(f'supplier_yield_obs_{i}', \n            alpha=α[i], beta=β[i],\n            observed=d)\n    \n    trace = pm.sample()\n\n/Users/twiecki/projects/pymc/pymc3/sampling.py:466: FutureWarning: In an upcoming release, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n  warnings.warn(\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [β, α]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 23 seconds.\nThere were 2 divergences after tuning. Increase `target_accept` or reparameterize.\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\n    \n        \n      \n      100.00% [4000/4000 00:09&lt;00:00 Sampling 2 chains, 2 divergences]\n    \n    \n\n\n\n# make sure convergence looks good\naz.plot_energy(trace);\n\n/Users/twiecki/miniconda3/envs/pymc3theano/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn("
  },
  {
    "objectID": "blog/supply_chain.html#generate-possible-future-scenarios",
    "href": "blog/supply_chain.html#generate-possible-future-scenarios",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Generate possible future scenarios",
    "text": "Generate possible future scenarios\nIn order to perform Bayesian Decision Making we need an estimate of what the future might look like. As we are in a generative framework this is trivial: we just need to sample from the posterior predictive distribution of our model which generates new data based on our estimated posteriors.\n\nwith model:\n    post_pred = pm.sample_posterior_predictive(trace, 1000)\n\n/Users/twiecki/projects/pymc/pymc3/sampling.py:1688: UserWarning: samples parameter is smaller than nchains times ndraws, some draws and/or chains may not be represented in the returned posterior predictive sample\n  warnings.warn(\n\n\n\n    \n        \n      \n      100.00% [1000/1000 00:21&lt;00:00]\n    \n    \n\n\n\nsupplier_yield_post_pred = pd.DataFrame({k: v[:, 1] for k, v in post_pred.items()})\ndata_tidy = supplier_yield_post_pred.unstack().to_frame('yield')\ndata_tidy.index = data_tidy.index.set_names(['supplier', 'obs'])\ng = sns.FacetGrid(data=data_tidy.reset_index().dropna(), col='supplier')\ng.map(sns.distplot, 'yield', kde=False);\n\n/Users/twiecki/miniconda3/envs/pymc3theano/lib/python3.8/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\nThis plot shows, given the data and our model, what we can expect to observe. Note that these predictions take the uncertainty into account. For supplier 2 we have a lot of uncertainty because we only observed very few data points.\nGiven these estimates we can write a function that converts the orders we place to each supplier, the yield we assume for each one, and what their prices are.\n\ndef calc_yield_and_price(orders, \n                         supplier_yield=np.array([.9, .5, .8]),\n                         prices=PRICES\n                        ):\n    orders = np.asarray(orders)\n    \n    full_yield = np.sum(supplier_yield * orders)\n    price_per_item = np.sum(orders * prices) / np.sum(orders)\n    \n    return full_yield, price_per_item\n\ncalc_yield_and_price([100, 60, 60])\n\n(168.0, 160.0)\n\n\nSo given these (randomly picked) order amounts to each supplier and some deterministic yield, we would receive 168 functioning engines at an effective price of 160 BTC each."
  },
  {
    "objectID": "blog/supply_chain.html#bayesian-decision-making",
    "href": "blog/supply_chain.html#bayesian-decision-making",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Bayesian Decision Making",
    "text": "Bayesian Decision Making\nNow we have to actually do the optimization. First, we need to specify our objective function which will compute the total yield and effective price given a posterior predictive sample. Once we have that and our demand (also a sample from that distribution), we can compute our loss. As we have a distribution over possible scenarios, we compute the loss for each one and return the distribution.\n\ndef objective(orders, supplier_yield=supplier_yield_post_pred,\n              demand_samples=demand_samples, max_order_size=MAX_ORDER_SIZE):\n    orders = np.asarray(orders)\n    losses = []\n    \n    # Negative orders are impossible, indicated by np.inf\n    if np.any(orders &lt; 0):\n        return np.inf\n    # Ordering more than the supplier can ship is also impossible\n    if np.any(orders &gt; MAX_ORDER_SIZE):\n        return np.inf\n    \n    # Iterate over post pred samples provided in supplier_yield\n    for i, supplier_yield_sample in supplier_yield.iterrows():\n        full_yield, price_per_item = calc_yield_and_price(\n            orders,\n            supplier_yield=supplier_yield_sample\n        )\n        \n        # evaluate loss over each sample with one sample from the demand distribution\n        loss_i = loss(full_yield, demand_samples[i], price_per_item)\n        \n        losses.append(loss_i)\n        \n    return np.asarray(losses)\n\nGreat, we have all our required functions, let’s put things into an optimizer and see what happens.\n\nfrom scipy import optimize\n\n\n# parameters for the optimization, we're just including the max order sizes as bounds\nbounds = [(0, max_order) for max_order in MAX_ORDER_SIZE]\nstarting_value = [50., 50., 50.]\n\n\n# minimize the expected loss under all possible scenarios\nopt_stoch = optimize.minimize(lambda *args: np.mean(objective(*args)), \n                              starting_value, \n                              bounds=bounds)\n\n\nprint('Optimal order amount from every supplier = {}'.format(np.ceil(opt_stoch.x)))\n\nOptimal order amount from every supplier = [ 0. 55. 96.]\n\n\n\nprint('Total order amount from all suppliers = {}'.format(np.ceil(np.sum(opt_stoch.x))))\n\nTotal order amount from all suppliers = 150.0\n\n\nGreat, we did it! Excitedly you go to your manager and tell her the amazing model you built and the optimal order amounts. Unfortunately, she is not impressed and asks “that’s some fancy technique, but I’m not convinced this is actually better than what we currently use which is to just take the means of the yield distribution for each supplier.”"
  },
  {
    "objectID": "blog/supply_chain.html#evaluation",
    "href": "blog/supply_chain.html#evaluation",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Evaluation",
    "text": "Evaluation\nSlightly discouraged you go back to your desk and wonder why life is so unfair and you actually have to prove that things work and why “but it’s Bayesian!” is not as convincing an argument as you hoped for. After some deep reflection you come to the conclusion that your manager might have a point and that additional complexity must be warranted and demonstrably better. To build a more compelling case, you decide to compare the naive method of just using the means to your fancy method in terms of expected profit in a simulation study.\nInstead of samples from the posterior predictive, we can just pass a single sample – the mean – into our objective function.\n\nsupplier_yield_mean = pd.DataFrame([np.mean(d) for d in data]).T\nsupplier_yield_mean\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.918735\n0.558903\n0.896244\n\n\n\n\n\n\n\nAs well as the demand we expect on average (100). This way we can still use the above objective function but the loop will just run once.\n\nopt_non_stoch = optimize.minimize(lambda *args: objective(*args, \n                                                          supplier_yield=supplier_yield_mean, \n                                                          demand_samples=[100]), \n                                  starting_value, \n                                  bounds=bounds)\n\n\nprint('Optimal order amount from every supplier = {}'.format(np.ceil(opt_non_stoch.x)))\n\nOptimal order amount from every supplier = [42. 46. 42.]\n\n\n\nprint('Total order amount from all suppliers = {}'.format(np.ceil(np.sum(opt_non_stoch.x))))\n\nTotal order amount from all suppliers = 128.0\n\n\nThe results are certainly different. The full Bayesian treatment seems to dislike our high-cost but high-quality supplier. It also orders more in total (probably to account for the lower yield of the other two suppliers). But which one is actually better in terms of our profit?\nTo answer that question, we will generate new data from our true generative model and compute the profit in each new scenario given the order amounts from the two optimizations.\n\nnp.random.seed(123)\ndata_new = []\nfor supplier_yield, supplier_yield_sd, n_obs in zip(SUPPLIER_YIELD, SUPPLIER_YIELD_SD, N_OBS):\n    data_new.append(pm.Beta.dist(mu=supplier_yield, sd=supplier_yield_sd, shape=1000).random())\ndata_new = pd.DataFrame(data_new).T\ndata_new.head().add_prefix(\"Supplier \")\n\n\n\n\n\n\n\n\nSupplier 0\nSupplier 1\nSupplier 2\n\n\n\n\n0\n0.880298\n0.752686\n0.997934\n\n\n1\n0.698046\n0.307304\n0.971085\n\n\n2\n0.676807\n0.534287\n0.891209\n\n\n3\n0.943773\n0.666368\n0.975907\n\n\n4\n0.911538\n0.457898\n0.556483\n\n\n\n\n\n\n\n\nneg_loss_stoch = -objective(opt_stoch.x, supplier_yield=data_new) / demand_samples\nneg_loss_non_stoch = -objective(opt_non_stoch.x, supplier_yield=data_new) / demand_samples\nsns.distplot(neg_loss_stoch, label='stochastic', kde=False)\nplt.axvline(np.mean(neg_loss_stoch), label='expected stochastic')\nsns.distplot(neg_loss_non_stoch, label='non-stochastic', kde=False)\nplt.axvline(np.mean(neg_loss_non_stoch), color='orange', label='expected non-stochastic')\nplt.legend(); plt.xlabel('Profit (negative loss)'); plt.ylabel('Occurances');\n\n/Users/twiecki/miniconda3/envs/pymc3theano/lib/python3.8/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\nprint('Expected profit of Bayesian model = %.2f BTC' % np.mean(neg_loss_stoch))\nprint('Expected profit of naive model = %.2f BTC' % np.mean(neg_loss_non_stoch))\nprint('Expected value of including uncertainty = %.2f BTC' % (np.mean(neg_loss_stoch) - np.mean(neg_loss_non_stoch)))\n\nExpected profit of Bayesian model = 346.32 BTC\nExpected profit of naive model = 317.01 BTC\nExpected value of including uncertainty = 29.31 BTC\n\n\nYour manager is very pleased that you finally speak her language and demonstrated an expected 10% increase in profit, which translates to millions of additional profit over a year at the scale the spaceport operates on. For your demonstrated ability to understand business requirements you get promoted to Chief Bayesian Officer."
  },
  {
    "objectID": "blog/supply_chain.html#summary",
    "href": "blog/supply_chain.html#summary",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Summary",
    "text": "Summary\nAs you can see, once we have a Bayesian model and an objective function we can apply Bayesian Decision Theory to make better decisions. Why better? While there is a mathematical proof that shows this to be optimal, there are also more practical and intuitive reasons. The first major reason is that we do not just optimize over the most likely future scenario, but all possible future scenarios.\nIn addition, as we did the optimization just using samples rather probability distributions we don’t have to do any integration (see this great blog post for an introduction). This gives us huge flexibility in our approach as we can arbitrarily extend our models but still use the same framework. For example, we could use the Prophet forecasting model to forecast demand more accurately. Or we could extend our yield estimation model to be hierarchical. If you want to play around with this, you can download the notebook."
  },
  {
    "objectID": "blog/supply_chain.html#acknowledgements",
    "href": "blog/supply_chain.html#acknowledgements",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Peadar Coyle for useful feedback on an earlier draft. Also thanks to the Patreon supporters, specifically Jonathan Ng and Richard Craib. If you enjoyed this post, please consider supporting me on Patreon.\nFinally, thanks to Ravin Kumar who teamed up with me to write this blog post. He used to optimize supply chains at SpaceX so he helped a lot with making sure our examples are actually relevant. He also has an amazing blog."
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html",
    "href": "blog/new_in_pymc3_3.1.html",
    "title": "Dummy - New in pymc 3.1",
    "section": "",
    "text": "We recently released PyMC3 3.1 after the first stable 3.0 release in January 2017. You can update either via pip install pymc3 or via conda install -c conda-forge pymc3.\nA lot is happening in PyMC3-land. One thing I am particularily proud of is the developer community we have built. We now have around 10 active core contributors from the US, Germany, Russia, Japan and Switzerland. Specifically, since 3.0, Adrian Seyboldt, Junpeng Lao and Hannes Bathke have joined the team. Moreover, we have 3 Google Summer of Code students: Maxime Kochurov, who is working on Variational Inference; Bill Engels, who is working on Gaussian Processes, and Bhargav Srinivasa is implementing Riemannian HMC.\nMoreover, PyMC3 is being seeing increased adoption in academia, as well as in industry.\nHere, I want to highlight some of the new features of PyMC3 3.1."
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#discourse-forum-better-docs",
    "href": "blog/new_in_pymc3_3.1.html#discourse-forum-better-docs",
    "title": "Dummy - New in pymc 3.1",
    "section": "Discourse forum + better docs",
    "text": "Discourse forum + better docs\nTo facilitate the community building process and give users a place to ask questions we have a launched a discourse forum: http://discourse.pymc.io. Bug reports should still onto the Github issue tracker, but for all PyMC3 questions or modeling discussions, please use the discourse forum.\nThere are also some improvements to the documentation. Mainly, a quick-start to the general PyMC3 API, and a quick-start to the variational API."
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#gaussian-processes",
    "href": "blog/new_in_pymc3_3.1.html#gaussian-processes",
    "title": "Dummy - New in pymc 3.1",
    "section": "Gaussian Processes",
    "text": "Gaussian Processes\nPyMC3 now as high-level support for GPs which allow for very flexible non-linear curve-fitting (among other things). This work was mainly done by Bill Engels with help from Chris Fonnesbeck. Here, we highlight the basic API, but for more information see the full introduction.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cmap\ncm = cmap.inferno\n\nimport numpy as np\nimport scipy as sp\nimport seaborn as sns\n\nimport theano\nimport theano.tensor as tt\nimport theano.tensor.nlinalg\n\nimport pymc3 as pm\n\n\nnp.random.seed(20090425)\nn = 20\nX = pm.floatX(np.sort(3*np.random.rand(n))[:,None])\n\n# generate fake data from GP with white noise (with variance sigma2)\ny = pm.floatX(\n    np.array([ 1.36653628,  1.15196999,  0.82142869,  0.85243384,  0.63436304,\n               0.14416139,  0.09454237,  0.32878065,  0.51946622,  0.58603513,\n               0.46938673,  0.63876778,  0.48415033,  1.28011185,  1.52401102,\n               1.38430047,  0.47455605, -0.21110139, -0.49443319, -0.25518805])\n)\n\n\nZ = pm.floatX(np.linspace(0, 3, 100)[:, None])\n\nwith pm.Model() as model:\n    # priors on the covariance function hyperparameters and noise\n    l = pm.Uniform('l', 0, 10)\n    log_s2_f = pm.Uniform('log_s2_f', lower=-10, upper=5)\n    log_s2_n = pm.Uniform('log_s2_n', lower=-10, upper=5)\n    f_cov = tt.exp(log_s2_f) * pm.gp.cov.ExpQuad(1, l)\n\n    # Instantiate GP\n    y_obs = pm.gp.GP('y_obs', cov_func=f_cov, sigma=tt.exp(log_s2_n), \n                     observed={'X': X, 'Y': y})\n    \n    trace = pm.sample()\n    \n    # Draw samples from GP\n    gp_samples = pm.gp.sample_gp(trace, y_obs, Z, samples=50, random_seed=42)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using ADVI...\nAverage Loss = 27.649:   6%|▌         | 12091/200000 [00:09&lt;02:15, 1386.23it/s]\nConvergence archived at 12100\nInterrupted at 12,100 [6%]: Average Loss = 9,348\n100%|██████████| 1000/1000 [00:20&lt;00:00, 49.74it/s]\n100%|██████████| 50/50 [00:12&lt;00:00,  3.93it/s]\n\n\n\nfig, ax = plt.subplots(figsize=(9, 5))\n\n[ax.plot(Z, x, color=cm(0.3), alpha=0.3) for x in gp_samples]\n# overlay the observed data\nax.plot(X, y, 'ok', ms=10);\nax.set(xlabel=\"x\", ylabel=\"f(x)\", title=\"Posterior predictive distribution\");"
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#improvements-to-nuts",
    "href": "blog/new_in_pymc3_3.1.html#improvements-to-nuts",
    "title": "Dummy - New in pymc 3.1",
    "section": "Improvements to NUTS",
    "text": "Improvements to NUTS\nNUTS is now identical to Stan’s implementation and also much much faster. In addition, Adrian Seyboldt added higher-order integrators, which promise to be more efficient in higher dimensions, and sampler statistics that help identify problems with NUTS sampling.\nIn addition, we changed the default kwargs of pm.sample(). By default, the sampler is run for 500 iterations with tuning enabled (you can change this with the tune kwarg), these samples are then discarded from the returned trace. Moreover, if no arguments are specified, sample() will draw 500 samples in addition to the tuning samples. So for almost all models, just calling pm.sample() should be sufficient.\n\nwith pm.Model():\n    mu1 = pm.Normal(\"mu1\", mu=0, sd=1, shape=1000)\n    trace = pm.sample(discard_tuned_samples=False) # do not remove tuned samples for the plot below\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using ADVI...\nAverage Loss = 7.279:  14%|█▍        | 28648/200000 [00:08&lt;00:53, 3176.47it/s] \nConvergence archived at 28900\nInterrupted at 28,900 [14%]: Average Loss = 8.9536\n100%|██████████| 1000/1000 [00:03&lt;00:00, 263.60it/s]\n\n\ntrace now has a bunch of extra parameters pertaining to statistics of the sampler:\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(trace['step_size_bar']); ax.set(xlabel='iteration', ylabel='step size');"
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#variational-inference",
    "href": "blog/new_in_pymc3_3.1.html#variational-inference",
    "title": "Dummy - New in pymc 3.1",
    "section": "Variational Inference",
    "text": "Variational Inference\nMaxim “Ferrine” Kochurov has done outstanding contributions to improve support for Variational Inference. Essentially, Ferrine has implemented Operator Variational Inference (OPVI) which is a framework to express many existing VI approaches in a modular fashion. He has also made it much easier to supply mini-batches. See here for a full overview of the capabilities.\nSpecifically, PyMC3 supports the following VI methods: * Auto-diff Variational Inference (ADVI) mean-field * ADVI full rank * Stein Variational Gradient Descent (SVGD) * Armortized SVGD\nIn addition, Ferrine is making great progress on adding Flows which allows learning very flexible transformations of the VI approximation to learn more complex (i.e. non-normal) posterior distributions.\n\nx = np.random.randn(10000)\nx_mini = pm.Minibatch(x, batch_size=100)\n\nwith pm.Model():\n    mu = pm.Normal('x', mu=0, sd=1)\n    sd = pm.HalfNormal('sd', sd=1)\n    obs = pm.Normal('obs', mu=mu, sd=sd, observed=x_mini)\n    vi_est = pm.fit() # Run ADVI\n    vi_trace = vi_est.sample() # sample from VI posterior\n\nAverage Loss = 149.38: 100%|██████████| 10000/10000 [00:01&lt;00:00, 9014.13it/s]\nFinished [100%]: Average Loss = 149.33\n\n\n\nplt.plot(vi_est.hist)\nplt.ylabel('ELBO'); plt.xlabel('iteration');\n\n\n\n\n\npm.traceplot(vi_trace);\n\n\n\n\nAs you can see, we have also added a new high-level API in the spirit of sample: pymc3.fit() with many configuration options:\n\nhelp(pm.fit)\n\nHelp on function fit in module pymc3.variational.inference:\n\nfit(n=10000, local_rv=None, method='advi', model=None, random_seed=None, start=None, inf_kwargs=None, **kwargs)\n    Handy shortcut for using inference methods in functional way\n    \n    Parameters\n    ----------\n    n : `int`\n        number of iterations\n    local_rv : dict[var-&gt;tuple]\n        mapping {model_variable -&gt; local_variable (:math:`\\mu`, :math:`\\rho`)}\n        Local Vars are used for Autoencoding Variational Bayes\n        See (AEVB; Kingma and Welling, 2014) for details\n    method : str or :class:`Inference`\n        string name is case insensitive in {'advi', 'fullrank_advi', 'advi-&gt;fullrank_advi', 'svgd', 'asvgd'}\n    model : :class:`pymc3.Model`\n        PyMC3 model for inference\n    random_seed : None or int\n        leave None to use package global RandomStream or other\n        valid value to create instance specific one\n    inf_kwargs : dict\n        additional kwargs passed to :class:`Inference`\n    start : `Point`\n        starting point for inference\n    \n    Other Parameters\n    ----------------\n    frac : `float`\n        if method is 'advi-&gt;fullrank_advi' represents advi fraction when training\n    kwargs : kwargs\n        additional kwargs for :func:`Inference.fit`\n    \n    Returns\n    -------\n    :class:`Approximation`\n\n\n\nSVGD for example is an algorithm that updates multiple particles and is thus well suited for multi-modal posteriors.\n\nwith pm.Model():\n    pm.NormalMixture('m', \n                     mu=np.array([0., .5]), \n                     w=np.array([.4, .6]), \n                     sd=np.array([.1, .1]))\n    vi_est = pm.fit(method='SVGD')\n    vi_est = vi_est.sample(5000)\n\n100%|██████████| 10000/10000 [00:24&lt;00:00, 407.10it/s]\n\n\n\nsns.distplot(vi_est['m'])\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x12f335208&gt;"
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#cholesky-factorization",
    "href": "blog/new_in_pymc3_3.1.html#cholesky-factorization",
    "title": "Dummy - New in pymc 3.1",
    "section": "Cholesky factorization",
    "text": "Cholesky factorization\nThere is a nice trick to covariance estimation using the Cholesky decomposition for increased efficiency and numerical stability. The MvNormal distribution now accepts a Cholesky-factored covariance matrix. In addition, the LKJ prior has been changed to provide the Cholesky covariance matrix. Thus, if you are estimating covariances, definitely use this much improved parameterization.\n\nn_dim = 5\ndata = np.random.randn(100, n_dim)\n\nwith pm.Model() as model:\n    # Note that we access the distribution for the standard\n    # deviations, and do not create a new random variable.\n    sd_dist = pm.HalfCauchy.dist(beta=2.5)\n    packed_chol = pm.LKJCholeskyCov('chol_cov', eta=2, n=n_dim, \n                                    sd_dist=sd_dist)\n    chol = pm.expand_packed_triangular(n_dim, packed_chol, lower=True)\n\n    # Define a new MvNormal with the given covariance\n    vals = pm.MvNormal('vals', mu=np.zeros(n_dim), \n                       chol=chol, shape=n_dim,\n                       observed=data)\n    trace = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using ADVI...\nAverage Loss = 716.37:   8%|▊         | 15309/200000 [00:05&lt;01:06, 2768.80it/s]\nConvergence archived at 15400\nInterrupted at 15,400 [7%]: Average Loss = 2,171.4\n100%|██████████| 1000/1000 [00:06&lt;00:00, 160.64it/s]"
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#live-trace-to-see-sampling-in-real-time",
    "href": "blog/new_in_pymc3_3.1.html#live-trace-to-see-sampling-in-real-time",
    "title": "Dummy - New in pymc 3.1",
    "section": "Live-trace to see sampling in real-time",
    "text": "Live-trace to see sampling in real-time\nThis one is really cool, you can watch the trace evolve while its sampling using pm.sample(live_plot=True). Contributed by David Brochart. See here for full docs."
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#better-display-of-random-variables",
    "href": "blog/new_in_pymc3_3.1.html#better-display-of-random-variables",
    "title": "Dummy - New in pymc 3.1",
    "section": "Better display of random variables",
    "text": "Better display of random variables\nWe now make use of the fancy display features of the Jupyter Notebook to provide a nicer view of RVs:\n\nwith pm.Model():\n    μ = pm.Normal('μ', mu=0, sd=1)\n    σ = pm.HalfNormal('σ', sd=1)\n    γ = pm.Normal('γ', mu=μ, sd=σ, observed=np.random.randn(100))\n\n\nγ\n\n\\(γ \\sim \\text{Normal}(\\mathit{mu}=μ, \\mathit{sd}=f(σ))\\)\n\n\nYou can create greek letters in the Jupyter Notebook by typing the \\(\\LaTeX\\) command and hitting tab: ."
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#gpu-support-experimental",
    "href": "blog/new_in_pymc3_3.1.html#gpu-support-experimental",
    "title": "Dummy - New in pymc 3.1",
    "section": "GPU support (experimental)",
    "text": "GPU support (experimental)\nWhile still experimental, we have made a lot of progress towards fully supporting float32 throughout. If you set floatX = float32 in your .theanorc, cast all your input data to float32 (e.g. by using pm.floatX() for automatic casting), you should get much faster inference. If you set the backend to use the GPU, you should get a nice speed-up on the right types of models. Please report any successes or failures in this regard."
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#other-useful-packages",
    "href": "blog/new_in_pymc3_3.1.html#other-useful-packages",
    "title": "Dummy - New in pymc 3.1",
    "section": "Other useful packages",
    "text": "Other useful packages\nThese are not part of PyMC3 3.1 but build on it and should be of interest to many users.\n\nBayesian Deep Learning with Gelato\nGelato bridges PyMC3 and Lasagne, a library to easily build Neural Networks similar to Keras. Building Bayesian convolution neural networks and estimating them using VI has never been simpler. See here for an example on MNIST.\n\n\nBuilding hierarchical GLMs with Bambi\nBambi is a new package on top of PyMC3 (they also recently added a Stan backend) which allows creation of complex, hierarchical GLMs with very intuitive syntax, e.g.:\nmodel.fit('rt ~ condition', random=['condition|subject', '1|stimulus'], samples=5000, chains=2)."
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#looking-towards-pymc3-3.2",
    "href": "blog/new_in_pymc3_3.1.html#looking-towards-pymc3-3.2",
    "title": "Dummy - New in pymc 3.1",
    "section": "Looking towards PyMC3 3.2",
    "text": "Looking towards PyMC3 3.2\nPyMC3 3.2 is already underway. Some of the features we are working on include: * faster sampling on the GPU, * Stochastic Gradient Fisher Scoring for scalable min-batch MCMC, * Normalizing Flows for flexible variational inference on non-normal posteriors, * scalable GPs, * support for the emcee sampler, and * Riemannian HMC for efficient sampling in high-dimensional complex spaces."
  },
  {
    "objectID": "blog/new_in_pymc3_3.1.html#on-my-own-behalf-patreon",
    "href": "blog/new_in_pymc3_3.1.html#on-my-own-behalf-patreon",
    "title": "Dummy - New in pymc 3.1",
    "section": "On my own behalf: Patreon",
    "text": "On my own behalf: Patreon\nI have recently created an account on Patreon where you can support me financially for writing blog posts. These allow me to devote more time to writing posts so if you find this blog useful, please consider supporting me.\nThanks specifically to Jonathan Ng for pledging."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html",
    "href": "blog/MCMC-sampling-for-dummies.html",
    "title": "MCMC sampling for dummies",
    "section": "",
    "text": "When I give talks about probabilistic programming and Bayesian statistics, I usually gloss over the details of how inference is actually performed, treating it as a black box essentially. The beauty of probabilistic programming is that you actually don’t have to understand how the inference works in order to build models, but it certainly helps.\nWhen I presented a new Bayesian model to Quantopian’s CEO, Fawce, who wasn’t trained in Bayesian stats but is eager to understand it, he started to ask about the part I usually gloss over: “Thomas, how does the inference actually work? How do we get these magical samples from the posterior?”.\nNow I could have said: “Well that’s easy, MCMC generates samples from the posterior distribution by constructing a reversible Markov-chain that has as its equilibrium distribution the target posterior distribution. Questions?”.\nThat statement is correct, but is it useful? My pet peeve with how math and stats are taught is that no one ever tells you about the intuition behind the concepts (which is usually quite simple) but only hands you some scary math. This is certainly the way I was taught and I had to spend countless hours banging my head against the wall until that euraka moment came about. Usually things weren’t as scary or seemingly complex once I deciphered what it meant.\nThis blog post is an attempt at trying to explain the intuition behind MCMC sampling (specifically, the random-walk Metropolis algorithm). Critically, we’ll be using code examples rather than formulas or math-speak. Eventually you’ll need that but I personally think it’s better to start with the an example and build the intuition before you move on to the math."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html#the-problem-and-its-unintuitive-solution",
    "href": "blog/MCMC-sampling-for-dummies.html#the-problem-and-its-unintuitive-solution",
    "title": "MCMC sampling for dummies",
    "section": "The problem and its unintuitive solution",
    "text": "The problem and its unintuitive solution\nLets take a look at Bayes formula:\n\\[P(\\theta|x) = \\frac{P(x|\\theta) P(\\theta)}{P(x)}\\]\nWe have \\(P(\\theta|x)\\), the probability of our model parameters \\(\\theta\\) given the data \\(x\\) and thus our quantity of interest. To compute this we multiply the prior \\(P(\\theta)\\) (what we think about \\(\\theta\\) before we have seen any data) and the likelihood \\(P(x|\\theta)\\), i.e. how we think our data is distributed. This nominator is pretty easy to solve for.\nHowever, lets take a closer look at the denominator. \\(P(x)\\) which is also called the evidence (i.e. the evidence that the data x was generated by this model). We can compute this quantity by integrating over all possible parameter values: \\[P(x) = \\int_\\Theta P(x, \\theta) \\, \\mathrm{d}\\theta\\]\nThis is the key difficulty with Bayes formula – while the formula looks innocent enough, for even slightly non-trivial models you just can’t compute the posterior in a closed-form way.\nNow we might say “OK, if we can’t solve something, could we try to approximate it? For example, if we could somehow draw samples from that posterior we can Monte Carlo approximate it.” Unfortunately, to directly sample from that distribution you not only have to solve Bayes formula, but also invert it, so that’s even harder.\nThen we might say “Well, instead let’s construct an ergodic, reversible Markov chain that has as an equilibrium distribution which matches our posterior distribution”. I’m just kidding, most people wouldn’t say that as it sounds bat-shit crazy. If you can’t compute it, can’t sample from it, then constructing that Markov chain with all these properties must be even harder.\nThe surprising insight though is that this is actually very easy and there exist a general class of algorithms that do this called Markov chain Monte Carlo (constructing a Markov chain to do Monte Carlo approximation)."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html#setting-up-the-problem",
    "href": "blog/MCMC-sampling-for-dummies.html#setting-up-the-problem",
    "title": "MCMC sampling for dummies",
    "section": "Setting up the problem",
    "text": "Setting up the problem\nFirst, lets import our modules.\n\n%matplotlib inline\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import norm\n\nsns.set_style('white')\nsns.set_context('talk')\n\nnp.random.seed(123)\n\nLets generate some data: 20 points from a normal centered around zero. Our goal will be to estimate the posterior of the mean mu (we’ll assume that we know the standard deviation to be 1).\n\ndata = np.random.randn(20)\n\n\nax = plt.subplot()\nsns.distplot(data, kde=False, ax=ax)\n_ = ax.set(title='Histogram of observed data', xlabel='x', ylabel='# observations');\n\n\n\n\nNext, we have to define our model. In this simple case, we will assume that this data is normal distributed, i.e. the likelihood of the model is normal. As you know, a normal distribution has two parameters – mean \\(\\mu\\) and standard deviation \\(\\sigma\\). For simplicity, we’ll assume we know that \\(\\sigma = 1\\) and we’ll want to infer the posterior for \\(\\mu\\). For each parameter we want to infer, we have to chose a prior. For simplicity, lets also assume a Normal distribution as a prior for \\(\\mu\\). Thus, in stats speak our model is:\n\\[\\mu \\sim \\text{Normal}(0, 1)\\\\\nx|\\mu \\sim \\text{Normal}(x; \\mu, 1)\\]\nWhat is convenient, is that for this model, we actually can compute the posterior analytically. That’s because for a normal likelihood with known standard deviation, the normal prior for mu is conjugate (conjugate here means that our posterior will follow the same distribution as the prior), so we know that our posterior for \\(\\mu\\) is also normal. We can easily look up on wikipedia how we can compute the parameters of the posterior. For a mathemtical derivation of this, see here.\n\ndef calc_posterior_analytical(data, x, mu_0, sigma_0):\n    sigma = 1.\n    n = len(data)\n    mu_post = (mu_0 / sigma_0**2 + data.sum() / sigma**2) / (1. / sigma_0**2 + n / sigma**2)\n    sigma_post = (1. / sigma_0**2 + n / sigma**2)**-1\n    return norm(mu_post, np.sqrt(sigma_post)).pdf(x)\n\nax = plt.subplot()\nx = np.linspace(-1, 1, 500)\nposterior_analytical = calc_posterior_analytical(data, x, 0., 1.)\nax.plot(x, posterior_analytical)\nax.set(xlabel='mu', ylabel='belief', title='Analytical posterior');\nsns.despine()\n\n\n\n\nThis shows our quantity of interest, the probability of \\(\\mu\\)’s values after having seen the data, taking our prior information into account. Lets assume, however, that our prior wasn’t conjugate and we couldn’t solve this by hand which is usually the case."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html#explaining-mcmc-sampling-with-code",
    "href": "blog/MCMC-sampling-for-dummies.html#explaining-mcmc-sampling-with-code",
    "title": "MCMC sampling for dummies",
    "section": "Explaining MCMC sampling with code",
    "text": "Explaining MCMC sampling with code\nNow on to the sampling logic. At first, you find starting parameter position (can be randomly chosen), lets fix it arbitrarily to:\nmu_current = 1.\nThen, you propose to move (jump) from that position somewhere else (that’s the Markov part). You can be very dumb or very sophisticated about how you come up with that proposal. The Metropolis sampler is very dumb and just takes a sample from a normal distribution (no relationship to the normal we assume for the model) centered around your current mu value (i.e. mu_current) with a certain standard deviation (proposal_width) that will determine how far you propose jumps (here we’re use scipy.stats.norm):\nproposal = norm(mu_current, proposal_width).rvs()\nNext, you evaluate whether that’s a good place to jump to or not. If the resulting normal distribution with that proposed mu explaines the data better than your old mu, you’ll definitely want to go there. What does “explains the data better” mean? We quantify fit by computing the probability of the data, given the likelihood (normal) with the proposed parameter values (proposed mu and a fixed sigma = 1). This can easily be computed by calculating the probability for each data point using scipy.stats.normal(mu, sigma).pdf(data) and then multiplying the individual probabilities, i.e. compute the likelihood (usually you would use log probabilities but we omit this here):\nlikelihood_current = norm(mu_current, 1).pdf(data).prod()\nlikelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n        \n# Compute prior probability of current and proposed mu        \nprior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\nprior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n\n# Nominator of Bayes formula\np_current = likelihood_current * prior_current\np_proposal = likelihood_proposal * prior_proposal\nUp until now, we essentially have a hill-climbing algorithm that would just propose movements into random directions and only accept a jump if the mu_proposal has higher likelihood than mu_current. Eventually we’ll get to mu = 0 (or close to it) from where no more moves will be possible. However, we want to get a posterior so we’ll also have to sometimes accept moves into the other direction. The key trick is by dividing the two probabilities,\np_accept = p_proposal / p_current\nwe get an acceptance probability. You can already see that if p_proposal is larger, that probability will be &gt; 1 and we’ll definitely accept. However, if p_current is larger, say twice as large, there’ll be a 50% chance of moving there:\naccept = np.random.rand() &lt; p_accept\n\nif accept:\n    # Update position\n    cur_pos = proposal\nThis simple procedure gives us samples from the posterior.\n\nWhy does this make sense?\nTaking a step back, note that the above acceptance ratio is the reason this whole thing works out and we get around the integration. We can show this by computing the acceptance ratio over the normalized posterior and seeing how it’s equivalent to the acceptance ratio of the unnormalized posterior (lets say \\(\\mu_0\\) is our current position, and \\(\\mu\\) is our proposal):\n\\[ \\frac{\\frac{P(x|\\mu) P(\\mu)}{P(x)}}{\\frac{P(x|\\mu_0) P(\\mu_0)}{P(x)}} = \\frac{P(x|\\mu) P(\\mu)}{P(x|\\mu_0) P(\\mu_0)}\\]\nIn words, dividing the posterior of proposed parameter setting by the posterior of the current parameter setting, \\(P(x)\\) – that nasty quantity we can’t compute – gets canceled out. So you can intuit that we’re actually dividing the full posterior at one position by the full posterior at another position (no magic here). That way, we are visiting regions of high posterior probability relatively more often than those of low posterior probability.\n\n\nPutting it all together\n\ndef sampler(data, samples=4, mu_init=.5, proposal_width=.5, plot=False, mu_prior_mu=0, mu_prior_sd=1.):\n    mu_current = mu_init\n    posterior = [mu_current]\n    for i in range(samples):\n        # suggest new position\n        mu_proposal = norm(mu_current, proposal_width).rvs()\n\n        # Compute likelihood by multiplying probabilities of each data point\n        likelihood_current = norm(mu_current, 1).pdf(data).prod()\n        likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n        \n        # Compute prior probability of current and proposed mu        \n        prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n        prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n        \n        p_current = likelihood_current * prior_current\n        p_proposal = likelihood_proposal * prior_proposal\n        \n        # Accept proposal?\n        p_accept = p_proposal / p_current\n        \n        # Usually would include prior probability, which we neglect here for simplicity\n        accept = np.random.rand() &lt; p_accept\n        \n        if plot:\n            plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accept, posterior, i)\n        \n        if accept:\n            # Update position\n            mu_current = mu_proposal\n        \n        posterior.append(mu_current)\n        \n    return np.array(posterior)\n\n# Function to display\ndef plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accepted, trace, i):\n    from copy import copy\n    trace = copy(trace)\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(16, 4))\n    fig.suptitle('Iteration %i' % (i + 1))\n    x = np.linspace(-3, 3, 5000)\n    color = 'g' if accepted else 'r'\n        \n    # Plot prior\n    prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n    prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n    prior = norm(mu_prior_mu, mu_prior_sd).pdf(x)\n    ax1.plot(x, prior)\n    ax1.plot([mu_current] * 2, [0, prior_current], marker='o', color='b')\n    ax1.plot([mu_proposal] * 2, [0, prior_proposal], marker='o', color=color)\n    ax1.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n                 arrowprops=dict(arrowstyle=\"-&gt;\", lw=2.))\n    ax1.set(ylabel='Probability Density', title='current: prior(mu=%.2f) = %.2f\\nproposal: prior(mu=%.2f) = %.2f' % (mu_current, prior_current, mu_proposal, prior_proposal))\n    \n    # Likelihood\n    likelihood_current = norm(mu_current, 1).pdf(data).prod()\n    likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n    y = norm(loc=mu_proposal, scale=1).pdf(x)\n    sns.distplot(data, kde=False, norm_hist=True, ax=ax2)\n    ax2.plot(x, y, color=color)\n    ax2.axvline(mu_current, color='b', linestyle='--', label='mu_current')\n    ax2.axvline(mu_proposal, color=color, linestyle='--', label='mu_proposal')\n    #ax2.title('Proposal {}'.format('accepted' if accepted else 'rejected'))\n    ax2.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n                 arrowprops=dict(arrowstyle=\"-&gt;\", lw=2.))\n    ax2.set(title='likelihood(mu=%.2f) = %.2f\\nlikelihood(mu=%.2f) = %.2f' % (mu_current, 1e14*likelihood_current, mu_proposal, 1e14*likelihood_proposal))\n    \n    # Posterior\n    posterior_analytical = calc_posterior_analytical(data, x, mu_prior_mu, mu_prior_sd)\n    ax3.plot(x, posterior_analytical)\n    posterior_current = calc_posterior_analytical(data, mu_current, mu_prior_mu, mu_prior_sd)\n    posterior_proposal = calc_posterior_analytical(data, mu_proposal, mu_prior_mu, mu_prior_sd)\n    ax3.plot([mu_current] * 2, [0, posterior_current], marker='o', color='b')\n    ax3.plot([mu_proposal] * 2, [0, posterior_proposal], marker='o', color=color)\n    ax3.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n                 arrowprops=dict(arrowstyle=\"-&gt;\", lw=2.))\n    #x3.set(title=r'prior x likelihood $\\propto$ posterior')\n    ax3.set(title='posterior(mu=%.2f) = %.5f\\nposterior(mu=%.2f) = %.5f' % (mu_current, posterior_current, mu_proposal, posterior_proposal))\n    \n    if accepted:\n        trace.append(mu_proposal)\n    else:\n        trace.append(mu_current)\n    ax4.plot(trace)\n    ax4.set(xlabel='iteration', ylabel='mu', title='trace')\n    plt.tight_layout()\n    #plt.legend()"
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html#visualizing-mcmc",
    "href": "blog/MCMC-sampling-for-dummies.html#visualizing-mcmc",
    "title": "MCMC sampling for dummies",
    "section": "Visualizing MCMC",
    "text": "Visualizing MCMC\nTo visualize the sampling, we’ll create plots for some quantities that are computed. Each row below is a single iteration through our Metropolis sampler.\nThe first columns is our prior distribution – what our belief about \\(\\mu\\) is before seeing the data. You can see how the distribution is static and we only plug in our \\(\\mu\\) proposals. The vertical lines represent our current \\(\\mu\\) in blue and our proposed \\(\\mu\\) in either red or green (rejected or accepted, respectively).\nThe 2nd column is our likelihood and what we are using to evaluate how good our model explains the data. You can see that the likelihood function changes in response to the proposed \\(\\mu\\). The blue histogram which is our data. The solid line in green or red is the likelihood with the currently proposed mu. Intuitively, the more overlap there is between likelihood and data, the better the model explains the data and the higher the resulting probability will be. The dotted line of the same color is the proposed mu and the dotted blue line is the current mu.\nThe 3rd column is our posterior distribution. Here I am displaying the normalized posterior but as we found out above, we can just multiply the prior value for the current and proposed \\(\\mu\\)’s by the likelihood value for the two \\(\\mu\\)’s to get the unnormalized posterior values (which we use for the actual computation), and divide one by the other to get our acceptance probability.\nThe 4th column is our trace (i.e. the posterior samples of \\(\\mu\\) we’re generating) where we store each sample irrespective of whether it was accepted or rejected (in which case the line just stays constant).\nNote that we always move to relatively more likely \\(\\mu\\) values (in terms of their posterior density), but only sometimes to relatively less likely \\(\\mu\\) values, as can be seen in iteration 14 (the iteration number can be found at the top center of each row).\n\nnp.random.seed(123)\nsampler(data, samples=8, mu_init=-1., plot=True);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the magic of MCMC is that you just have to do that for a long time, and the samples that are generated in this way come from the posterior distribution of your model. There is a rigorous mathematical proof that guarantees this which I won’t go into detail here.\nTo get a sense of what this produces, lets draw a lot of samples and plot them.\n\nposterior = sampler(data, samples=15000, mu_init=1.)\nfig, ax = plt.subplots()\nax.plot(posterior)\n_ = ax.set(xlabel='sample', ylabel='mu');\n\n\n\n\nThis is usually called the trace. To now get an approximation of the posterior (the reason why we’re doing all this), we simply take the histogram of this trace. It’s important to keep in mind that although this looks similar to the data we sampled above to fit the model, the two are completely separate. The below plot represents our belief in mu. In this case it just happens to also be normal but for a different model, it could have a completely different shape than the likelihood or prior.\n\nax = plt.subplot()\n\nsns.distplot(posterior[500:], ax=ax, label='estimated posterior')\nx = np.linspace(-.5, .5, 500)\npost = calc_posterior_analytical(data, x, 0, 1)\nax.plot(x, post, 'g', label='analytic posterior')\n_ = ax.set(xlabel='mu', ylabel='belief');\nax.legend();\n\n\n\n\nAs you can see, by following the above procedure, we get samples from the same distribution as what we derived analytically."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html#proposal-width",
    "href": "blog/MCMC-sampling-for-dummies.html#proposal-width",
    "title": "MCMC sampling for dummies",
    "section": "Proposal width",
    "text": "Proposal width\nAbove we set the proposal width to 0.5. That turned out to be a pretty good value. In general you don’t want the width to be too narrow because your sampling will be inefficient as it takes a long time to explore the whole parameter space and shows the typical random-walk behavior:\n\nposterior_small = sampler(data, samples=5000, mu_init=1., proposal_width=.01)\nfig, ax = plt.subplots()\nax.plot(posterior_small);\n_ = ax.set(xlabel='sample', ylabel='mu');\n\n\n\n\nBut you also don’t want it to be so large that you never accept a jump:\n\nposterior_large = sampler(data, samples=5000, mu_init=1., proposal_width=3.)\nfig, ax = plt.subplots()\nax.plot(posterior_large); plt.xlabel('sample'); plt.ylabel('mu');\n_ = ax.set(xlabel='sample', ylabel='mu');\n\n\n\n\nNote, however, that we are still sampling from our target posterior distribution here as guaranteed by the mathematical proof, just less efficiently:\n\nsns.distplot(posterior_small[1000:], label='Small step size')\nsns.distplot(posterior_large[1000:], label='Large step size');\n_ = plt.legend();\n\n\n\n\nWith more samples this will eventually look like the true posterior. The key is that we want our samples to be independent of each other which cleary isn’t the case here. Thus, one common metric to evaluate the efficiency of our sampler is the autocorrelation – i.e. how correlated a sample i is to sample i-1, i-2, etc:\n\nfrom pymc3.stats import autocorr\nlags = np.arange(1, 100)\nfig, ax = plt.subplots()\nax.plot(lags, [autocorr(posterior_large, l) for l in lags], label='large step size')\nax.plot(lags, [autocorr(posterior_small, l) for l in lags], label='small step size')\nax.plot(lags, [autocorr(posterior, l) for l in lags], label='medium step size')\nax.legend(loc=0)\n_ = ax.set(xlabel='lag', ylabel='autocorrelation', ylim=(-.1, 1))\n\n\n\n\nObviously we want to have a smart way of figuring out the right step width automatically. One common method is to keep adjusting the proposal width so that roughly 50% proposals are rejected."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html#extending-to-more-complex-models",
    "href": "blog/MCMC-sampling-for-dummies.html#extending-to-more-complex-models",
    "title": "MCMC sampling for dummies",
    "section": "Extending to more complex models",
    "text": "Extending to more complex models\nNow you can easily imagine that we could also add a sigma parameter for the standard-deviation and follow the same procedure for this second parameter. In that case, we would be generating proposals for mu and sigma but the algorithm logic would be nearly identical. Or, we could have data from a very different distribution like a Binomial and still use the same algorithm and get the correct posterior. That’s pretty cool and a huge benefit of probabilistic programming: Just define the model you want and let MCMC take care of the inference.\nFor example, the below model can be written in PyMC3 quite easily. Below we also use the Metropolis sampler (which automatically tunes the proposal width) and see that we get identical results. Feel free to play around with this and change the distributions. For more information, as well as more complex examples, see the PyMC3 documentation.\n\nimport pymc3 as pm\n\nwith pm.Model():\n    mu = pm.Normal('mu', 0, 1)\n    sigma = 1.\n    returns = pm.Normal('returns', mu=mu, sd=sigma, observed=data)\n    \n    step = pm.Metropolis()\n    trace = pm.sample(15000, step)\n    \nsns.distplot(trace[2000:]['mu'], label='PyMC3 sampler');\nsns.distplot(posterior[500:], label='Hand-written sampler');\nplt.legend();\n\n [-----------------100%-----------------] 15000 of 15000 complete in 1.7 sec"
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html#conclusions",
    "href": "blog/MCMC-sampling-for-dummies.html#conclusions",
    "title": "MCMC sampling for dummies",
    "section": "Conclusions",
    "text": "Conclusions\nWe glossed over a lot of detail which is certainly important but there are many other posts that deal with that. Here, we really wanted to communicate the idea of MCMC and the Metropolis sampler. Hopefully you will have gathered some intuition which will equip you to read one of the more technical introductions to this topic.\nOther, more fancy, MCMC algorithms like Hamiltonian Monte Carlo actually work very similar to this, they are just much more clever in proposing where to jump next.\nThis blog post was written in a Jupyter Notebook, you can find the underlying NB with all its code here."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies.html#support-me-on-patreon",
    "href": "blog/MCMC-sampling-for-dummies.html#support-me-on-patreon",
    "title": "MCMC sampling for dummies",
    "section": "Support me on Patreon",
    "text": "Support me on Patreon\nFinally, if you enjoyed this blog post, consider supporting me on Patreon which allows me to devote more time to writing new blog posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "<code><span style='color: #8ecae6'>while</span>&nbsp;<span style='color: #ffb703'>my_mcmc:</span><br>&nbsp;&nbsp;gently(samples)</code>",
    "section": "",
    "text": "It is already doing it\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom-Walk Bayesian Deep Networks: Dealing with Non-Stationary Data\n\n\n\n\n\n\nThomas Wiecki & Ravin Kumar\n\n\nJan 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Bayesian Decision Making to Optimize Supply Chains\n\n\n\n\n\n\nThomas Wiecki & Ravin Kumar\n\n\nJan 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Bayesian Decision Making to Optimize Supply Chains\n\n\n\n\n\n\nThomas Wiecki & Ravin Kumar\n\n\nJan 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nAn intuitive, visual guide to copulas\n\n\n\n\n\n\nThomas Wiecki\n\n\nMay 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nAn intuitive, visual guide to copulas\n\n\n\n\n\n\nThomas Wiecki\n\n\nMay 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC sampling for dummies\n\n\n\n\n\n\nThomas Wiecki\n\n\nNov 10, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC sampling for dummies\n\n\n\n\n\n\nThomas Wiecki\n\n\nNov 10, 2015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Best Of Both Worlds: Hierarchical Linear Regression in PyMC3\n\n\n\n\n\n\nThomas Wiecki & Danne Elbers\n\n\nMar 17, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Best Of Both Worlds: Hierarchical Linear Regression in PyMC3\n\n\n\n\n\n\nThomas Wiecki & Danne Elbers\n\n\nMar 17, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDummy - New in pymc 3.1\n\n\n\n\n\n\nThomas Wiecki & Danne Elbers\n\n\nMar 1, 2014\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2021-02-23-saving-the-world.html",
    "href": "blog/2021-02-23-saving-the-world.html",
    "title": "<code><span style='color: #8ecae6'>while</span>&nbsp;<span style='color: #ffb703'>my_mcmc:</span><br>&nbsp;&nbsp;gently(samples)</code>",
    "section": "",
    "text": "Title: Introducing PyMC Labs: Saving the World with Bayesian Modeling date: 2021-02-23 10:00 comments: true draft: true slug: intro_pymc_labs\nAfter I left Quantopian in 2020, something interesting happened: various companies contacted me inquiring about consulting to help them with their PyMC3 models.\nUsually, I don’t hear how people are using PyMC3 – they mostly show up on GitHub or Discourse when something isn’t working right. So, hearing about all these really cool projects was quite exciting. However, I couldn’t possibly take all of these projects on by myself.\nThus, it was time to assemble a team of the most badass Bayesian modelers the world had ever seen – the Bayesian Avengers, if you will. Fortunately, I did not have to venture far, as PyMC3 had already attracted exactly these types of people.\nThis brings me to the Big Announcement: For the last few months, we have quietly been building PyMC Labs, a Bayesian modeling consultancy. We have an amazing team consisting of three neuroscience PhDs, mathematicians, social scientists, a SpaceX rocket scientist, and the host of the famous ‘Learning Bayesian Statistics’ podcast. All of us are united in our mission:\nDoes this sound a bit grandiose? Probably. Is this true? I firmly believe it is. There are so many important problems the world faces today – from climate change to COVID19, from education to poverty – and Bayesian modeling can play a critical role in solving these problems. Let me explain why."
  },
  {
    "objectID": "blog/2021-02-23-saving-the-world.html#it-is-already-doing-it",
    "href": "blog/2021-02-23-saving-the-world.html#it-is-already-doing-it",
    "title": "<code><span style='color: #8ecae6'>while</span>&nbsp;<span style='color: #ffb703'>my_mcmc:</span><br>&nbsp;&nbsp;gently(samples)</code>",
    "section": "It is already doing it",
    "text": "It is already doing it\nI would not have imagined it when I started contributing to PyMC, but the science PyMC3 has directly enabled ranges from climate science and biology to astronomy and zoology, and everything in between.\nFor instance, it was used to predict the spread of COVID19 in a recent Science paper, as well as track the reproduction factor in real-time. In both cases, the benefit of PyMC3 was its ease-of-use and the ability to integrate scientific domain knowledge and get honest uncertainty estimation in a highly volatile and uncertain situation.\nNow I know you’re very observant and I hear you thinking: “wait a minute, those benefits of Bayesian modeling sound quite general, so why would they be only valid for epidemiology?”. And indeed they aren’t! For similar benefits, PyMC3 is also used to find planets outside of our solar system and detect earthquakes. One of my coworkers here at PyMC Labs uses it for electoral and political forecasting, because polls are noisy, scarce and need to be completed by domain knowledge – one of the perfect settings for Bayesian inference!\nWith all of this, at the time of writing, the PyMC3 paper has been cited over 930 times and is in the top 10 most cited articles of the entire PeerJ journal."
  },
  {
    "objectID": "blog/2021-02-23-saving-the-world.html#solving-business-problems",
    "href": "blog/2021-02-23-saving-the-world.html#solving-business-problems",
    "title": "<code><span style='color: #8ecae6'>while</span>&nbsp;<span style='color: #ffb703'>my_mcmc:</span><br>&nbsp;&nbsp;gently(samples)</code>",
    "section": "Solving Business Problems",
    "text": "Solving Business Problems\nBeyond scientific research, I find that PyMC3 is the perfect tool to also solve various business problems. And indeed it’s already successfully used in production at companies as big and diverse as SpaceX, Roche, Netflix, Deliveroo and HelloFresh.\nThis diversity means that the PyMC Labs team intervenes to, for instance, build complex models from the latest finance research; optimize supply chains for food delivery; build software from top to bottom for pharmaceutical applications; speed up and extend models for the farm tech industry; train and enhance any data science team’s Bayesian stats capacities, etc."
  },
  {
    "objectID": "blog/2021-02-23-saving-the-world.html#prediction-vs-inference",
    "href": "blog/2021-02-23-saving-the-world.html#prediction-vs-inference",
    "title": "<code><span style='color: #8ecae6'>while</span>&nbsp;<span style='color: #ffb703'>my_mcmc:</span><br>&nbsp;&nbsp;gently(samples)</code>",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\nAs data science has exploded in the last decade I have always been surprised by the over-emphasis on prediction-focused machine learning. For far too long, it has been hailed as the solution to most of our data science problems.\nI believe that the potential of this is way overblown. Not because it doesn’t work – algorithms like deep nets or random forests are extremely powerful at extracting non-linear predictive patterns from large data sets – but rather because most data science problems are not simple prediction but rather inference problems.\nIn addition, we often already have a lot of knowledge about our problem: knowledge of certain structure in our data set (like nested data, that some variables relate to some but not other parameters) and knowledge of which range of values we expect certain parameters of our model to fall into. Prediction-focused ML does not allow us to include any of this information, that’s why it requires so much data.\nWith Bayesian statistics, we don’t have to learn everything from data as we translate this knowledge into a custom model. Thus, rather than changing our problem to fit the solution, as is common with ML, we can tailor the solution to best solve the problem at hand. I like to compare this with Playmobil vs Lego:\n\nPlaymobil just gives you a single toy you can’t change while Lego (i.e Bayes here) gives you building blocks to build the toy you actually want. In Bayesian modeling, these building blocks are probability distributions.\nBut how do you do this in practice? This is where PyMC3 comes in, as it allows you to specify your models as Python code and automatically estimate it without requiring manual mathematical derivations. Due to recent theoretical and technological advances, this also runs quickly and scales to complex models on large(ish) data sets."
  },
  {
    "objectID": "blog/2021-02-23-saving-the-world.html#serving-our-mission",
    "href": "blog/2021-02-23-saving-the-world.html#serving-our-mission",
    "title": "<code><span style='color: #8ecae6'>while</span>&nbsp;<span style='color: #ffb703'>my_mcmc:</span><br>&nbsp;&nbsp;gently(samples)</code>",
    "section": "Serving our mission",
    "text": "Serving our mission\nSo how do we best make progress on our mission?\nFirst, we will continue to make PyMC3 the best, most user-friendly and scalable Bayesian modeling package out there. We are well set up to do this, having a friendly API, a huge user-base, and a large developer team of over 20 active members. With our renewed focus on PyMC3 on Theano with a JAX backend all our resources will go towards this goal.\nSecond, our new PyMC consultancy will support this endeavour. It allows us to directly help clients use these powerful, customizable methods to solve their business problems, thereby increasing adoption and recognition. As a great side effect, these client projects also help us find things that need to be fixed, improved or optimized in PyMC3, thereby lifting all (Bayesian) boats instead of just the happy fews’.\nSo far, this has been an incredibly rewarding and exhilarating journey. Even though it is still early, we are learning a lot about which areas Bayesian modeling is particularly well suited for but also what would make PyMC3 even better. Without spoiling a future blog post that will go into more detail about what we have learned applying these methods, the best use-cases include (but aren’t limited to) incorporating domain knowledge, building bespoke models and quantifying uncertainty around estimates.\nSounds familiar? If you or your company has a problem for which prediction-based ML is not a good fit, I’d love to talk to you at thomas.wiecki@pymc-labs.io. This is just the beginning and I hope you will join us on this marvelous journey."
  },
  {
    "objectID": "blog/supply_chain - Copy.html",
    "href": "blog/supply_chain - Copy.html",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "",
    "text": "2019 Thomas Wiecki & Ravin Kumar\nOne big problem I often observe with data science is that it often falls short of having a true impact on the bottom line of the business. The reasons for this are manifold but I firmly believe that Bayesian modeling solves many of them. Two highlight just two:\nHowever, there still is a gap when it comes to having these outputs (posterior distributions) actually have an impact. You might be quite pleased with yourself when you give your manager not just a point-estimate for how much budget they should allocate but instead a posterior distribution, but when they make their budget allocation decision, they still need to input just a single number.\nThe solution to this problem, and one we often use for client projects at PyMC Labs, is to also model the decision making process and incorporate the model estimate directly into that. In brief, by defining a loss function that maps business decisions to outcomes we can use an optimizer to find the best decision(s) not only under the most likely scenario, but under all plausible scenarios. This not only moves Bayesian modeling from something that informs a decision to something that makes a decision, it also allows you - the modeler - to communicate your results in the only language business cares about:\nIn this blog post we want to demonstrate this powerful method with a the general problem of supply chain optimization, an area where Bayesian statistics can have a big impact.\n%matplotlib inline\n\nimport pymc3 as pm\nimport theano.tensor as tt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport arviz as az\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')"
  },
  {
    "objectID": "blog/supply_chain - Copy.html#supply-chain-optimization-to-operate-a-spaceport",
    "href": "blog/supply_chain - Copy.html#supply-chain-optimization-to-operate-a-spaceport",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Supply chain optimization to operate a spaceport",
    "text": "Supply chain optimization to operate a spaceport\nIt is the year 12119 (under our new Human Era calendar), humanity has become a space-faring civilization. Bayesians and Frequentists still argue about which statistics are better. You are a Data Scientist at a spaceport called PyMC-X (why did you think we have a rocket for our logo?) that sends humans to the moon, Mars, Europa, and the ISS 5.0 in Saturn’s orbit. You are tasked with managing the supply chain to keep the rockets running.\nWhile the rockets themselves are reusable, we need a new rocket engine for every launch. There are three suppliers we can order engines from. These suppliers have different prices, different yields (i.e. how many of their shipped engines are functional), and different maximum order amounts. While we know the prices and order sizes, the true yield we do not know and can only estimate from past orders. However, as we will use simulated data here, we will include the unobservable parameters SUPPLIER_YIELD and SUPPLIER_YIELD_SD. In reality we would not have access to this information.\n\nSUPPLIER_YIELD = np.array([.9, .5, .8]) # unknown\nSUPPLIER_YIELD_SD = np.array([.1, .2, .2]) # unknown\nPRICES = [220.0, 100.0, 120.0] # known\nMAX_ORDER_SIZE = [100, 80, 100] # known\n\nThe yield represents the percentage of engines that pass our stress tests (a faulty engine lead to the abort of the Space Shuttle launch STS-93 so this is an actual problem). Due to different manufacturing techniques, the yield varies quite a bit by supplier, which is also reflected in the price. As don’t know the true yield, we will have to estimate it from previous batches we ordered. In our example, we have ordered more times from certain suppliers than others. For example, as supplier 2 (third list item) only opened up recently, we only ordered twice from there:\n\nN_OBS = [30, 20, 2]\n\n\nnp.random.seed(100)\ndata = []\nfor supplier_yield, supplier_yield_sd, n_obs in zip(SUPPLIER_YIELD, SUPPLIER_YIELD_SD, N_OBS):\n    data.append(pm.Beta.dist(mu=supplier_yield, sd=supplier_yield_sd, shape=n_obs).random())\n    \ndata\n\n[array([0.978235  , 0.98946102, 0.99035051, 0.83762708, 0.66130327,\n        0.98785994, 0.85327018, 0.8500779 , 0.99913878, 0.89881072,\n        0.8175994 , 0.95181804, 0.91545214, 0.87137954, 0.96166603,\n        0.99033823, 0.96319861, 0.94124979, 0.96555922, 0.96606356,\n        0.92723444, 0.97736913, 0.86764773, 0.81749131, 0.98597604,\n        0.97980665, 0.77295709, 0.9584931 , 0.88875261, 0.99585613]),\n array([0.51788973, 0.67831661, 0.64888304, 0.61595363, 0.08634205,\n        0.72543455, 0.51883833, 0.5454235 , 0.30357696, 0.21743938,\n        0.54628383, 0.68559965, 0.28827533, 0.79246239, 0.65810975,\n        0.69059483, 0.59297579, 0.85482231, 0.38115298, 0.8296909 ]),\n array([0.89241857, 0.9000698 ])]\n\n\n\ndata_df = pd.DataFrame(data).T\ndata_tidy = data_df.unstack().to_frame('yield')\ndata_tidy.index = data_tidy.index.set_names(['supplier', 'obs'])\ng = sns.FacetGrid(data=data_tidy.reset_index().dropna(), col='supplier')\ng.map(sns.distplot, 'yield', kde=False);\n\n/Users/twiecki/miniconda3/envs/pymc3theano/lib/python3.8/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\nSo this is the data we have available to try and estimate the true yield of every supplier."
  },
  {
    "objectID": "blog/supply_chain - Copy.html#quick-note-on-the-generality-of-this-problem",
    "href": "blog/supply_chain - Copy.html#quick-note-on-the-generality-of-this-problem",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Quick note on the generality of this problem",
    "text": "Quick note on the generality of this problem\nWe mainly choose the spaceport example in a scifi setting because it’s fun, but the underlying problem is very general and the solution widely applicable. Almost every retailer (like Amazon) has this problem of deciding how much to order given yield and holding cost. A similar problem also occurs in insurance where you need to sell contracts which have some risk of becoming claims. Or in online advertising where you need to decide how much to bid on clicks given a budget. Even if you don’t work on these industries, the cost of any inefficiencies gets passed onto you, the customer! We also had a similar problem at Quantopian when deciding which algorithms to select in our fund and how much capital to deploy to each one. OK, back to optimizing the supply chain at PyMC-X!"
  },
  {
    "objectID": "blog/supply_chain - Copy.html#the-dynamics-of-operating-a-spaceport",
    "href": "blog/supply_chain - Copy.html#the-dynamics-of-operating-a-spaceport",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "The dynamics of operating a spaceport",
    "text": "The dynamics of operating a spaceport\nIn order to assess how many engines we need we first need to know how many rocket launches we can sell. If we buy too few we are leaving money on the table, if we buy too many we will have to put them in storage which costs money (HOLDING_COST). Let’s assume we can sell a rocket for 500 bitcoins (BTC) and it costs us 100 BTC in holding cost.\n\nSALES_PRICE = 500 \nHOLDING_COST = 100\n\nNext, let’s define our loss function which takes as inputs how many engines we have in stock, how many launches customers want, at what price we bought the engine, at what price we can sell the launch, and what the holding costs are per engine:\n\n@np.vectorize\ndef loss(in_stock, demand, buy_price, sales_price=SALES_PRICE, holding_cost=HOLDING_COST):\n    # How much do we earn per launch\n    margin = sales_price - buy_price\n    # Do we have more in stock than demanded?\n    if in_stock &gt; demand:\n        total_profit = demand * margin\n        # everything left over after demand was met goes into holding\n        total_holding_cost = (in_stock - demand) * holding_cost\n        reward = total_profit - total_holding_cost\n    else:\n        # Can only sell what we have in stock, no storage required\n        reward = in_stock * margin\n    \n    # Usually we minimize, so invert\n    return -reward\n\n\nin_stock = np.arange(0, 100)\nplt.scatter(in_stock, -loss(in_stock, 50, 50)); plt.axvline(50, c='k', ls='--', label='assumed demand');\nplt.xlabel('in stock'); plt.ylabel('profit (neg loss)'); sns.despine(); plt.legend();\n\n\n\n\nAs you can see, if customer demand is 50 launches, we maximize our profit if we have 50 engines in stock. Having fewer engines eats into our profits at a greater rate than ordering excess engines because in this setup our margins are larger than the holding cost.\nNext, we need our estimate of demand. As we have a long history of launches we have a pretty good idea of what the distribution looks like, but we will also assume that we don’t know the true underlying parameters and only have access to the samples:\n\ndemand_samples = stats.poisson(60, 40).rvs(1000)\nsns.distplot(demand_samples);\n\n\n\n\nWe can evaluate our objective function over every demand we observed historically (setting engines in stock to 100):\n\nplt.scatter(demand_samples, -loss(in_stock=100, demand=demand_samples, buy_price=10))\nplt.xlabel('demand'); plt.ylabel('profit (neg loss)'); plt.axvline(100, c='k', ls='--', label='assumed in stock');\nplt.legend();\n\n\n\n\nIn response to demand, the loss-function behaves differently: with less demand than what we have in stock, we earn less (because we sell fewer launches but also have to pay holding costs), but as demand exceeds the number of engines we have in stock our profit stays flat because we can’t sell more than what we have."
  },
  {
    "objectID": "blog/supply_chain - Copy.html#estimating-yield-with-a-bayesian-model",
    "href": "blog/supply_chain - Copy.html#estimating-yield-with-a-bayesian-model",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Estimating yield with a Bayesian model",
    "text": "Estimating yield with a Bayesian model\nLet’s use PyMC3 to build a model to estimate the yield of every engine supplier:\n\nwith pm.Model() as model:\n    # Priors on alpha and beta parameters for each supplier\n    α = pm.HalfNormal('α', sd=10., shape=3) + 1\n    β = pm.HalfNormal('β', sd=10., shape=3) + 1\n    \n    # Different likelihood for every supplier because we have different\n    # number of data points\n    for i, d in enumerate(data):\n        pm.Beta(f'supplier_yield_obs_{i}', \n            alpha=α[i], beta=β[i],\n            observed=d)\n    \n    trace = pm.sample()\n\n/Users/twiecki/projects/pymc/pymc3/sampling.py:466: FutureWarning: In an upcoming release, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n  warnings.warn(\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [β, α]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 23 seconds.\nThere were 2 divergences after tuning. Increase `target_accept` or reparameterize.\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\n    \n        \n      \n      100.00% [4000/4000 00:09&lt;00:00 Sampling 2 chains, 2 divergences]\n    \n    \n\n\n\n# make sure convergence looks good\naz.plot_energy(trace);\n\n/Users/twiecki/miniconda3/envs/pymc3theano/lib/python3.8/site-packages/arviz/data/io_pymc3.py:96: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  warnings.warn("
  },
  {
    "objectID": "blog/supply_chain - Copy.html#generate-possible-future-scenarios",
    "href": "blog/supply_chain - Copy.html#generate-possible-future-scenarios",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Generate possible future scenarios",
    "text": "Generate possible future scenarios\nIn order to perform Bayesian Decision Making we need an estimate of what the future might look like. As we are in a generative framework this is trivial: we just need to sample from the posterior predictive distribution of our model which generates new data based on our estimated posteriors.\n\nwith model:\n    post_pred = pm.sample_posterior_predictive(trace, 1000)\n\n/Users/twiecki/projects/pymc/pymc3/sampling.py:1688: UserWarning: samples parameter is smaller than nchains times ndraws, some draws and/or chains may not be represented in the returned posterior predictive sample\n  warnings.warn(\n\n\n\n    \n        \n      \n      100.00% [1000/1000 00:21&lt;00:00]\n    \n    \n\n\n\nsupplier_yield_post_pred = pd.DataFrame({k: v[:, 1] for k, v in post_pred.items()})\ndata_tidy = supplier_yield_post_pred.unstack().to_frame('yield')\ndata_tidy.index = data_tidy.index.set_names(['supplier', 'obs'])\ng = sns.FacetGrid(data=data_tidy.reset_index().dropna(), col='supplier')\ng.map(sns.distplot, 'yield', kde=False);\n\n/Users/twiecki/miniconda3/envs/pymc3theano/lib/python3.8/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\nThis plot shows, given the data and our model, what we can expect to observe. Note that these predictions take the uncertainty into account. For supplier 2 we have a lot of uncertainty because we only observed very few data points.\nGiven these estimates we can write a function that converts the orders we place to each supplier, the yield we assume for each one, and what their prices are.\n\ndef calc_yield_and_price(orders, \n                         supplier_yield=np.array([.9, .5, .8]),\n                         prices=PRICES\n                        ):\n    orders = np.asarray(orders)\n    \n    full_yield = np.sum(supplier_yield * orders)\n    price_per_item = np.sum(orders * prices) / np.sum(orders)\n    \n    return full_yield, price_per_item\n\ncalc_yield_and_price([100, 60, 60])\n\n(168.0, 160.0)\n\n\nSo given these (randomly picked) order amounts to each supplier and some deterministic yield, we would receive 168 functioning engines at an effective price of 160 BTC each."
  },
  {
    "objectID": "blog/supply_chain - Copy.html#bayesian-decision-making",
    "href": "blog/supply_chain - Copy.html#bayesian-decision-making",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Bayesian Decision Making",
    "text": "Bayesian Decision Making\nNow we have to actually do the optimization. First, we need to specify our objective function which will compute the total yield and effective price given a posterior predictive sample. Once we have that and our demand (also a sample from that distribution), we can compute our loss. As we have a distribution over possible scenarios, we compute the loss for each one and return the distribution.\n\ndef objective(orders, supplier_yield=supplier_yield_post_pred,\n              demand_samples=demand_samples, max_order_size=MAX_ORDER_SIZE):\n    orders = np.asarray(orders)\n    losses = []\n    \n    # Negative orders are impossible, indicated by np.inf\n    if np.any(orders &lt; 0):\n        return np.inf\n    # Ordering more than the supplier can ship is also impossible\n    if np.any(orders &gt; MAX_ORDER_SIZE):\n        return np.inf\n    \n    # Iterate over post pred samples provided in supplier_yield\n    for i, supplier_yield_sample in supplier_yield.iterrows():\n        full_yield, price_per_item = calc_yield_and_price(\n            orders,\n            supplier_yield=supplier_yield_sample\n        )\n        \n        # evaluate loss over each sample with one sample from the demand distribution\n        loss_i = loss(full_yield, demand_samples[i], price_per_item)\n        \n        losses.append(loss_i)\n        \n    return np.asarray(losses)\n\nGreat, we have all our required functions, let’s put things into an optimizer and see what happens.\n\nfrom scipy import optimize\n\n\n# parameters for the optimization, we're just including the max order sizes as bounds\nbounds = [(0, max_order) for max_order in MAX_ORDER_SIZE]\nstarting_value = [50., 50., 50.]\n\n\n# minimize the expected loss under all possible scenarios\nopt_stoch = optimize.minimize(lambda *args: np.mean(objective(*args)), \n                              starting_value, \n                              bounds=bounds)\n\n\nprint('Optimal order amount from every supplier = {}'.format(np.ceil(opt_stoch.x)))\n\nOptimal order amount from every supplier = [ 0. 55. 96.]\n\n\n\nprint('Total order amount from all suppliers = {}'.format(np.ceil(np.sum(opt_stoch.x))))\n\nTotal order amount from all suppliers = 150.0\n\n\nGreat, we did it! Excitedly you go to your manager and tell her the amazing model you built and the optimal order amounts. Unfortunately, she is not impressed and asks “that’s some fancy technique, but I’m not convinced this is actually better than what we currently use which is to just take the means of the yield distribution for each supplier.”"
  },
  {
    "objectID": "blog/supply_chain - Copy.html#evaluation",
    "href": "blog/supply_chain - Copy.html#evaluation",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Evaluation",
    "text": "Evaluation\nSlightly discouraged you go back to your desk and wonder why life is so unfair and you actually have to prove that things work and why “but it’s Bayesian!” is not as convincing an argument as you hoped for. After some deep reflection you come to the conclusion that your manager might have a point and that additional complexity must be warranted and demonstrably better. To build a more compelling case, you decide to compare the naive method of just using the means to your fancy method in terms of expected profit in a simulation study.\nInstead of samples from the posterior predictive, we can just pass a single sample – the mean – into our objective function.\n\nsupplier_yield_mean = pd.DataFrame([np.mean(d) for d in data]).T\nsupplier_yield_mean\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.918735\n0.558903\n0.896244\n\n\n\n\n\n\n\nAs well as the demand we expect on average (100). This way we can still use the above objective function but the loop will just run once.\n\nopt_non_stoch = optimize.minimize(lambda *args: objective(*args, \n                                                          supplier_yield=supplier_yield_mean, \n                                                          demand_samples=[100]), \n                                  starting_value, \n                                  bounds=bounds)\n\n\nprint('Optimal order amount from every supplier = {}'.format(np.ceil(opt_non_stoch.x)))\n\nOptimal order amount from every supplier = [42. 46. 42.]\n\n\n\nprint('Total order amount from all suppliers = {}'.format(np.ceil(np.sum(opt_non_stoch.x))))\n\nTotal order amount from all suppliers = 128.0\n\n\nThe results are certainly different. The full Bayesian treatment seems to dislike our high-cost but high-quality supplier. It also orders more in total (probably to account for the lower yield of the other two suppliers). But which one is actually better in terms of our profit?\nTo answer that question, we will generate new data from our true generative model and compute the profit in each new scenario given the order amounts from the two optimizations.\n\nnp.random.seed(123)\ndata_new = []\nfor supplier_yield, supplier_yield_sd, n_obs in zip(SUPPLIER_YIELD, SUPPLIER_YIELD_SD, N_OBS):\n    data_new.append(pm.Beta.dist(mu=supplier_yield, sd=supplier_yield_sd, shape=1000).random())\ndata_new = pd.DataFrame(data_new).T\ndata_new.head().add_prefix(\"Supplier \")\n\n\n\n\n\n\n\n\nSupplier 0\nSupplier 1\nSupplier 2\n\n\n\n\n0\n0.880298\n0.752686\n0.997934\n\n\n1\n0.698046\n0.307304\n0.971085\n\n\n2\n0.676807\n0.534287\n0.891209\n\n\n3\n0.943773\n0.666368\n0.975907\n\n\n4\n0.911538\n0.457898\n0.556483\n\n\n\n\n\n\n\n\nneg_loss_stoch = -objective(opt_stoch.x, supplier_yield=data_new) / demand_samples\nneg_loss_non_stoch = -objective(opt_non_stoch.x, supplier_yield=data_new) / demand_samples\nsns.distplot(neg_loss_stoch, label='stochastic', kde=False)\nplt.axvline(np.mean(neg_loss_stoch), label='expected stochastic')\nsns.distplot(neg_loss_non_stoch, label='non-stochastic', kde=False)\nplt.axvline(np.mean(neg_loss_non_stoch), color='orange', label='expected non-stochastic')\nplt.legend(); plt.xlabel('Profit (negative loss)'); plt.ylabel('Occurances');\n\n/Users/twiecki/miniconda3/envs/pymc3theano/lib/python3.8/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\nprint('Expected profit of Bayesian model = %.2f BTC' % np.mean(neg_loss_stoch))\nprint('Expected profit of naive model = %.2f BTC' % np.mean(neg_loss_non_stoch))\nprint('Expected value of including uncertainty = %.2f BTC' % (np.mean(neg_loss_stoch) - np.mean(neg_loss_non_stoch)))\n\nExpected profit of Bayesian model = 346.32 BTC\nExpected profit of naive model = 317.01 BTC\nExpected value of including uncertainty = 29.31 BTC\n\n\nYour manager is very pleased that you finally speak her language and demonstrated an expected 10% increase in profit, which translates to millions of additional profit over a year at the scale the spaceport operates on. For your demonstrated ability to understand business requirements you get promoted to Chief Bayesian Officer."
  },
  {
    "objectID": "blog/supply_chain - Copy.html#summary",
    "href": "blog/supply_chain - Copy.html#summary",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Summary",
    "text": "Summary\nAs you can see, once we have a Bayesian model and an objective function we can apply Bayesian Decision Theory to make better decisions. Why better? While there is a mathematical proof that shows this to be optimal, there are also more practical and intuitive reasons. The first major reason is that we do not just optimize over the most likely future scenario, but all possible future scenarios.\nIn addition, as we did the optimization just using samples rather probability distributions we don’t have to do any integration (see this great blog post for an introduction). This gives us huge flexibility in our approach as we can arbitrarily extend our models but still use the same framework. For example, we could use the Prophet forecasting model to forecast demand more accurately. Or we could extend our yield estimation model to be hierarchical. If you want to play around with this, you can download the notebook."
  },
  {
    "objectID": "blog/supply_chain - Copy.html#acknowledgements",
    "href": "blog/supply_chain - Copy.html#acknowledgements",
    "title": "Using Bayesian Decision Making to Optimize Supply Chains",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Peadar Coyle for useful feedback on an earlier draft. Also thanks to the Patreon supporters, specifically Jonathan Ng and Richard Craib. If you enjoyed this post, please consider supporting me on Patreon.\nFinally, thanks to Ravin Kumar who teamed up with me to write this blog post. He used to optimize supply chains at SpaceX so he helped a lot with making sure our examples are actually relevant. He also has an amazing blog."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html",
    "title": "MCMC sampling for dummies",
    "section": "",
    "text": "When I give talks about probabilistic programming and Bayesian statistics, I usually gloss over the details of how inference is actually performed, treating it as a black box essentially. The beauty of probabilistic programming is that you actually don’t have to understand how the inference works in order to build models, but it certainly helps.\nWhen I presented a new Bayesian model to Quantopian’s CEO, Fawce, who wasn’t trained in Bayesian stats but is eager to understand it, he started to ask about the part I usually gloss over: “Thomas, how does the inference actually work? How do we get these magical samples from the posterior?”.\nNow I could have said: “Well that’s easy, MCMC generates samples from the posterior distribution by constructing a reversible Markov-chain that has as its equilibrium distribution the target posterior distribution. Questions?”.\nThat statement is correct, but is it useful? My pet peeve with how math and stats are taught is that no one ever tells you about the intuition behind the concepts (which is usually quite simple) but only hands you some scary math. This is certainly the way I was taught and I had to spend countless hours banging my head against the wall until that euraka moment came about. Usually things weren’t as scary or seemingly complex once I deciphered what it meant.\nThis blog post is an attempt at trying to explain the intuition behind MCMC sampling (specifically, the random-walk Metropolis algorithm). Critically, we’ll be using code examples rather than formulas or math-speak. Eventually you’ll need that but I personally think it’s better to start with the an example and build the intuition before you move on to the math."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html#the-problem-and-its-unintuitive-solution",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html#the-problem-and-its-unintuitive-solution",
    "title": "MCMC sampling for dummies",
    "section": "The problem and its unintuitive solution",
    "text": "The problem and its unintuitive solution\nLets take a look at Bayes formula:\n\\[P(\\theta|x) = \\frac{P(x|\\theta) P(\\theta)}{P(x)}\\]\nWe have \\(P(\\theta|x)\\), the probability of our model parameters \\(\\theta\\) given the data \\(x\\) and thus our quantity of interest. To compute this we multiply the prior \\(P(\\theta)\\) (what we think about \\(\\theta\\) before we have seen any data) and the likelihood \\(P(x|\\theta)\\), i.e. how we think our data is distributed. This nominator is pretty easy to solve for.\nHowever, lets take a closer look at the denominator. \\(P(x)\\) which is also called the evidence (i.e. the evidence that the data x was generated by this model). We can compute this quantity by integrating over all possible parameter values: \\[P(x) = \\int_\\Theta P(x, \\theta) \\, \\mathrm{d}\\theta\\]\nThis is the key difficulty with Bayes formula – while the formula looks innocent enough, for even slightly non-trivial models you just can’t compute the posterior in a closed-form way.\nNow we might say “OK, if we can’t solve something, could we try to approximate it? For example, if we could somehow draw samples from that posterior we can Monte Carlo approximate it.” Unfortunately, to directly sample from that distribution you not only have to solve Bayes formula, but also invert it, so that’s even harder.\nThen we might say “Well, instead let’s construct an ergodic, reversible Markov chain that has as an equilibrium distribution which matches our posterior distribution”. I’m just kidding, most people wouldn’t say that as it sounds bat-shit crazy. If you can’t compute it, can’t sample from it, then constructing that Markov chain with all these properties must be even harder.\nThe surprising insight though is that this is actually very easy and there exist a general class of algorithms that do this called Markov chain Monte Carlo (constructing a Markov chain to do Monte Carlo approximation)."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html#setting-up-the-problem",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html#setting-up-the-problem",
    "title": "MCMC sampling for dummies",
    "section": "Setting up the problem",
    "text": "Setting up the problem\nFirst, lets import our modules.\n\n%matplotlib inline\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import norm\n\nsns.set_style('white')\nsns.set_context('talk')\n\nnp.random.seed(123)\n\nLets generate some data: 20 points from a normal centered around zero. Our goal will be to estimate the posterior of the mean mu (we’ll assume that we know the standard deviation to be 1).\n\ndata = np.random.randn(20)\n\n\nax = plt.subplot()\nsns.distplot(data, kde=False, ax=ax)\n_ = ax.set(title='Histogram of observed data', xlabel='x', ylabel='# observations');\n\n\n\n\nNext, we have to define our model. In this simple case, we will assume that this data is normal distributed, i.e. the likelihood of the model is normal. As you know, a normal distribution has two parameters – mean \\(\\mu\\) and standard deviation \\(\\sigma\\). For simplicity, we’ll assume we know that \\(\\sigma = 1\\) and we’ll want to infer the posterior for \\(\\mu\\). For each parameter we want to infer, we have to chose a prior. For simplicity, lets also assume a Normal distribution as a prior for \\(\\mu\\). Thus, in stats speak our model is:\n\\[\\mu \\sim \\text{Normal}(0, 1)\\\\\nx|\\mu \\sim \\text{Normal}(x; \\mu, 1)\\]\nWhat is convenient, is that for this model, we actually can compute the posterior analytically. That’s because for a normal likelihood with known standard deviation, the normal prior for mu is conjugate (conjugate here means that our posterior will follow the same distribution as the prior), so we know that our posterior for \\(\\mu\\) is also normal. We can easily look up on wikipedia how we can compute the parameters of the posterior. For a mathemtical derivation of this, see here.\n\ndef calc_posterior_analytical(data, x, mu_0, sigma_0):\n    sigma = 1.\n    n = len(data)\n    mu_post = (mu_0 / sigma_0**2 + data.sum() / sigma**2) / (1. / sigma_0**2 + n / sigma**2)\n    sigma_post = (1. / sigma_0**2 + n / sigma**2)**-1\n    return norm(mu_post, np.sqrt(sigma_post)).pdf(x)\n\nax = plt.subplot()\nx = np.linspace(-1, 1, 500)\nposterior_analytical = calc_posterior_analytical(data, x, 0., 1.)\nax.plot(x, posterior_analytical)\nax.set(xlabel='mu', ylabel='belief', title='Analytical posterior');\nsns.despine()\n\n\n\n\nThis shows our quantity of interest, the probability of \\(\\mu\\)’s values after having seen the data, taking our prior information into account. Lets assume, however, that our prior wasn’t conjugate and we couldn’t solve this by hand which is usually the case."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html#explaining-mcmc-sampling-with-code",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html#explaining-mcmc-sampling-with-code",
    "title": "MCMC sampling for dummies",
    "section": "Explaining MCMC sampling with code",
    "text": "Explaining MCMC sampling with code\nNow on to the sampling logic. At first, you find starting parameter position (can be randomly chosen), lets fix it arbitrarily to:\nmu_current = 1.\nThen, you propose to move (jump) from that position somewhere else (that’s the Markov part). You can be very dumb or very sophisticated about how you come up with that proposal. The Metropolis sampler is very dumb and just takes a sample from a normal distribution (no relationship to the normal we assume for the model) centered around your current mu value (i.e. mu_current) with a certain standard deviation (proposal_width) that will determine how far you propose jumps (here we’re use scipy.stats.norm):\nproposal = norm(mu_current, proposal_width).rvs()\nNext, you evaluate whether that’s a good place to jump to or not. If the resulting normal distribution with that proposed mu explaines the data better than your old mu, you’ll definitely want to go there. What does “explains the data better” mean? We quantify fit by computing the probability of the data, given the likelihood (normal) with the proposed parameter values (proposed mu and a fixed sigma = 1). This can easily be computed by calculating the probability for each data point using scipy.stats.normal(mu, sigma).pdf(data) and then multiplying the individual probabilities, i.e. compute the likelihood (usually you would use log probabilities but we omit this here):\nlikelihood_current = norm(mu_current, 1).pdf(data).prod()\nlikelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n        \n# Compute prior probability of current and proposed mu        \nprior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\nprior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n\n# Nominator of Bayes formula\np_current = likelihood_current * prior_current\np_proposal = likelihood_proposal * prior_proposal\nUp until now, we essentially have a hill-climbing algorithm that would just propose movements into random directions and only accept a jump if the mu_proposal has higher likelihood than mu_current. Eventually we’ll get to mu = 0 (or close to it) from where no more moves will be possible. However, we want to get a posterior so we’ll also have to sometimes accept moves into the other direction. The key trick is by dividing the two probabilities,\np_accept = p_proposal / p_current\nwe get an acceptance probability. You can already see that if p_proposal is larger, that probability will be &gt; 1 and we’ll definitely accept. However, if p_current is larger, say twice as large, there’ll be a 50% chance of moving there:\naccept = np.random.rand() &lt; p_accept\n\nif accept:\n    # Update position\n    cur_pos = proposal\nThis simple procedure gives us samples from the posterior.\n\nWhy does this make sense?\nTaking a step back, note that the above acceptance ratio is the reason this whole thing works out and we get around the integration. We can show this by computing the acceptance ratio over the normalized posterior and seeing how it’s equivalent to the acceptance ratio of the unnormalized posterior (lets say \\(\\mu_0\\) is our current position, and \\(\\mu\\) is our proposal):\n\\[ \\frac{\\frac{P(x|\\mu) P(\\mu)}{P(x)}}{\\frac{P(x|\\mu_0) P(\\mu_0)}{P(x)}} = \\frac{P(x|\\mu) P(\\mu)}{P(x|\\mu_0) P(\\mu_0)}\\]\nIn words, dividing the posterior of proposed parameter setting by the posterior of the current parameter setting, \\(P(x)\\) – that nasty quantity we can’t compute – gets canceled out. So you can intuit that we’re actually dividing the full posterior at one position by the full posterior at another position (no magic here). That way, we are visiting regions of high posterior probability relatively more often than those of low posterior probability.\n\n\nPutting it all together\n\ndef sampler(data, samples=4, mu_init=.5, proposal_width=.5, plot=False, mu_prior_mu=0, mu_prior_sd=1.):\n    mu_current = mu_init\n    posterior = [mu_current]\n    for i in range(samples):\n        # suggest new position\n        mu_proposal = norm(mu_current, proposal_width).rvs()\n\n        # Compute likelihood by multiplying probabilities of each data point\n        likelihood_current = norm(mu_current, 1).pdf(data).prod()\n        likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n        \n        # Compute prior probability of current and proposed mu        \n        prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n        prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n        \n        p_current = likelihood_current * prior_current\n        p_proposal = likelihood_proposal * prior_proposal\n        \n        # Accept proposal?\n        p_accept = p_proposal / p_current\n        \n        # Usually would include prior probability, which we neglect here for simplicity\n        accept = np.random.rand() &lt; p_accept\n        \n        if plot:\n            plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accept, posterior, i)\n        \n        if accept:\n            # Update position\n            mu_current = mu_proposal\n        \n        posterior.append(mu_current)\n        \n    return np.array(posterior)\n\n# Function to display\ndef plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accepted, trace, i):\n    from copy import copy\n    trace = copy(trace)\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(16, 4))\n    fig.suptitle('Iteration %i' % (i + 1))\n    x = np.linspace(-3, 3, 5000)\n    color = 'g' if accepted else 'r'\n        \n    # Plot prior\n    prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n    prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n    prior = norm(mu_prior_mu, mu_prior_sd).pdf(x)\n    ax1.plot(x, prior)\n    ax1.plot([mu_current] * 2, [0, prior_current], marker='o', color='b')\n    ax1.plot([mu_proposal] * 2, [0, prior_proposal], marker='o', color=color)\n    ax1.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n                 arrowprops=dict(arrowstyle=\"-&gt;\", lw=2.))\n    ax1.set(ylabel='Probability Density', title='current: prior(mu=%.2f) = %.2f\\nproposal: prior(mu=%.2f) = %.2f' % (mu_current, prior_current, mu_proposal, prior_proposal))\n    \n    # Likelihood\n    likelihood_current = norm(mu_current, 1).pdf(data).prod()\n    likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n    y = norm(loc=mu_proposal, scale=1).pdf(x)\n    sns.distplot(data, kde=False, norm_hist=True, ax=ax2)\n    ax2.plot(x, y, color=color)\n    ax2.axvline(mu_current, color='b', linestyle='--', label='mu_current')\n    ax2.axvline(mu_proposal, color=color, linestyle='--', label='mu_proposal')\n    #ax2.title('Proposal {}'.format('accepted' if accepted else 'rejected'))\n    ax2.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n                 arrowprops=dict(arrowstyle=\"-&gt;\", lw=2.))\n    ax2.set(title='likelihood(mu=%.2f) = %.2f\\nlikelihood(mu=%.2f) = %.2f' % (mu_current, 1e14*likelihood_current, mu_proposal, 1e14*likelihood_proposal))\n    \n    # Posterior\n    posterior_analytical = calc_posterior_analytical(data, x, mu_prior_mu, mu_prior_sd)\n    ax3.plot(x, posterior_analytical)\n    posterior_current = calc_posterior_analytical(data, mu_current, mu_prior_mu, mu_prior_sd)\n    posterior_proposal = calc_posterior_analytical(data, mu_proposal, mu_prior_mu, mu_prior_sd)\n    ax3.plot([mu_current] * 2, [0, posterior_current], marker='o', color='b')\n    ax3.plot([mu_proposal] * 2, [0, posterior_proposal], marker='o', color=color)\n    ax3.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n                 arrowprops=dict(arrowstyle=\"-&gt;\", lw=2.))\n    #x3.set(title=r'prior x likelihood $\\propto$ posterior')\n    ax3.set(title='posterior(mu=%.2f) = %.5f\\nposterior(mu=%.2f) = %.5f' % (mu_current, posterior_current, mu_proposal, posterior_proposal))\n    \n    if accepted:\n        trace.append(mu_proposal)\n    else:\n        trace.append(mu_current)\n    ax4.plot(trace)\n    ax4.set(xlabel='iteration', ylabel='mu', title='trace')\n    plt.tight_layout()\n    #plt.legend()"
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html#visualizing-mcmc",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html#visualizing-mcmc",
    "title": "MCMC sampling for dummies",
    "section": "Visualizing MCMC",
    "text": "Visualizing MCMC\nTo visualize the sampling, we’ll create plots for some quantities that are computed. Each row below is a single iteration through our Metropolis sampler.\nThe first columns is our prior distribution – what our belief about \\(\\mu\\) is before seeing the data. You can see how the distribution is static and we only plug in our \\(\\mu\\) proposals. The vertical lines represent our current \\(\\mu\\) in blue and our proposed \\(\\mu\\) in either red or green (rejected or accepted, respectively).\nThe 2nd column is our likelihood and what we are using to evaluate how good our model explains the data. You can see that the likelihood function changes in response to the proposed \\(\\mu\\). The blue histogram which is our data. The solid line in green or red is the likelihood with the currently proposed mu. Intuitively, the more overlap there is between likelihood and data, the better the model explains the data and the higher the resulting probability will be. The dotted line of the same color is the proposed mu and the dotted blue line is the current mu.\nThe 3rd column is our posterior distribution. Here I am displaying the normalized posterior but as we found out above, we can just multiply the prior value for the current and proposed \\(\\mu\\)’s by the likelihood value for the two \\(\\mu\\)’s to get the unnormalized posterior values (which we use for the actual computation), and divide one by the other to get our acceptance probability.\nThe 4th column is our trace (i.e. the posterior samples of \\(\\mu\\) we’re generating) where we store each sample irrespective of whether it was accepted or rejected (in which case the line just stays constant).\nNote that we always move to relatively more likely \\(\\mu\\) values (in terms of their posterior density), but only sometimes to relatively less likely \\(\\mu\\) values, as can be seen in iteration 14 (the iteration number can be found at the top center of each row).\n\nnp.random.seed(123)\nsampler(data, samples=8, mu_init=-1., plot=True);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the magic of MCMC is that you just have to do that for a long time, and the samples that are generated in this way come from the posterior distribution of your model. There is a rigorous mathematical proof that guarantees this which I won’t go into detail here.\nTo get a sense of what this produces, lets draw a lot of samples and plot them.\n\nposterior = sampler(data, samples=15000, mu_init=1.)\nfig, ax = plt.subplots()\nax.plot(posterior)\n_ = ax.set(xlabel='sample', ylabel='mu');\n\n\n\n\nThis is usually called the trace. To now get an approximation of the posterior (the reason why we’re doing all this), we simply take the histogram of this trace. It’s important to keep in mind that although this looks similar to the data we sampled above to fit the model, the two are completely separate. The below plot represents our belief in mu. In this case it just happens to also be normal but for a different model, it could have a completely different shape than the likelihood or prior.\n\nax = plt.subplot()\n\nsns.distplot(posterior[500:], ax=ax, label='estimated posterior')\nx = np.linspace(-.5, .5, 500)\npost = calc_posterior_analytical(data, x, 0, 1)\nax.plot(x, post, 'g', label='analytic posterior')\n_ = ax.set(xlabel='mu', ylabel='belief');\nax.legend();\n\n\n\n\nAs you can see, by following the above procedure, we get samples from the same distribution as what we derived analytically."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html#proposal-width",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html#proposal-width",
    "title": "MCMC sampling for dummies",
    "section": "Proposal width",
    "text": "Proposal width\nAbove we set the proposal width to 0.5. That turned out to be a pretty good value. In general you don’t want the width to be too narrow because your sampling will be inefficient as it takes a long time to explore the whole parameter space and shows the typical random-walk behavior:\n\nposterior_small = sampler(data, samples=5000, mu_init=1., proposal_width=.01)\nfig, ax = plt.subplots()\nax.plot(posterior_small);\n_ = ax.set(xlabel='sample', ylabel='mu');\n\n\n\n\nBut you also don’t want it to be so large that you never accept a jump:\n\nposterior_large = sampler(data, samples=5000, mu_init=1., proposal_width=3.)\nfig, ax = plt.subplots()\nax.plot(posterior_large); plt.xlabel('sample'); plt.ylabel('mu');\n_ = ax.set(xlabel='sample', ylabel='mu');\n\n\n\n\nNote, however, that we are still sampling from our target posterior distribution here as guaranteed by the mathematical proof, just less efficiently:\n\nsns.distplot(posterior_small[1000:], label='Small step size')\nsns.distplot(posterior_large[1000:], label='Large step size');\n_ = plt.legend();\n\n\n\n\nWith more samples this will eventually look like the true posterior. The key is that we want our samples to be independent of each other which cleary isn’t the case here. Thus, one common metric to evaluate the efficiency of our sampler is the autocorrelation – i.e. how correlated a sample i is to sample i-1, i-2, etc:\n\nfrom pymc3.stats import autocorr\nlags = np.arange(1, 100)\nfig, ax = plt.subplots()\nax.plot(lags, [autocorr(posterior_large, l) for l in lags], label='large step size')\nax.plot(lags, [autocorr(posterior_small, l) for l in lags], label='small step size')\nax.plot(lags, [autocorr(posterior, l) for l in lags], label='medium step size')\nax.legend(loc=0)\n_ = ax.set(xlabel='lag', ylabel='autocorrelation', ylim=(-.1, 1))\n\n\n\n\nObviously we want to have a smart way of figuring out the right step width automatically. One common method is to keep adjusting the proposal width so that roughly 50% proposals are rejected."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html#extending-to-more-complex-models",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html#extending-to-more-complex-models",
    "title": "MCMC sampling for dummies",
    "section": "Extending to more complex models",
    "text": "Extending to more complex models\nNow you can easily imagine that we could also add a sigma parameter for the standard-deviation and follow the same procedure for this second parameter. In that case, we would be generating proposals for mu and sigma but the algorithm logic would be nearly identical. Or, we could have data from a very different distribution like a Binomial and still use the same algorithm and get the correct posterior. That’s pretty cool and a huge benefit of probabilistic programming: Just define the model you want and let MCMC take care of the inference.\nFor example, the below model can be written in PyMC3 quite easily. Below we also use the Metropolis sampler (which automatically tunes the proposal width) and see that we get identical results. Feel free to play around with this and change the distributions. For more information, as well as more complex examples, see the PyMC3 documentation.\n\nimport pymc3 as pm\n\nwith pm.Model():\n    mu = pm.Normal('mu', 0, 1)\n    sigma = 1.\n    returns = pm.Normal('returns', mu=mu, sd=sigma, observed=data)\n    \n    step = pm.Metropolis()\n    trace = pm.sample(15000, step)\n    \nsns.distplot(trace[2000:]['mu'], label='PyMC3 sampler');\nsns.distplot(posterior[500:], label='Hand-written sampler');\nplt.legend();\n\n [-----------------100%-----------------] 15000 of 15000 complete in 1.7 sec"
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html#conclusions",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html#conclusions",
    "title": "MCMC sampling for dummies",
    "section": "Conclusions",
    "text": "Conclusions\nWe glossed over a lot of detail which is certainly important but there are many other posts that deal with that. Here, we really wanted to communicate the idea of MCMC and the Metropolis sampler. Hopefully you will have gathered some intuition which will equip you to read one of the more technical introductions to this topic.\nOther, more fancy, MCMC algorithms like Hamiltonian Monte Carlo actually work very similar to this, they are just much more clever in proposing where to jump next.\nThis blog post was written in a Jupyter Notebook, you can find the underlying NB with all its code here."
  },
  {
    "objectID": "blog/MCMC-sampling-for-dummies - Copy.html#support-me-on-patreon",
    "href": "blog/MCMC-sampling-for-dummies - Copy.html#support-me-on-patreon",
    "title": "MCMC sampling for dummies",
    "section": "Support me on Patreon",
    "text": "Support me on Patreon\nFinally, if you enjoyed this blog post, consider supporting me on Patreon which allows me to devote more time to writing new blog posts."
  },
  {
    "objectID": "blog/copulas - Copy.html",
    "href": "blog/copulas - Copy.html",
    "title": "An intuitive, visual guide to copulas",
    "section": "",
    "text": "People seemed to enjoy my intuitive and visual explanation of Markov chain Monte Carlo so I thought it would be fun to do another one, this time focused on copulas.\nIf you ask a statistician what a copula is they might say “a copula is a multivariate distribution \\(C(U_1, U_2, ...., U_n)\\) such that marginalizing gives \\(U_i \\sim \\operatorname{\\sf Uniform}(0, 1)\\)”. OK… wait, what? I personally really dislike these math-only explanations that make many concepts appear way more difficult to understand than they actually are and copulas are a great example of that. The name alone always seemed pretty daunting to me. However, they are actually quite simple so we’re going to try and demistify them a bit. At the end, we will see what role copulas played in the 2007-2008 Financial Crisis."
  },
  {
    "objectID": "blog/copulas - Copy.html#example-problem-case",
    "href": "blog/copulas - Copy.html#example-problem-case",
    "title": "An intuitive, visual guide to copulas",
    "section": "Example problem case",
    "text": "Example problem case\nLet’s start with an example problem case. Say we measure two variables that are non-normally distributed and correlated. For example, we look at various rivers and for every river we look at the maximum level of that river over a certain time-period. In addition, we also count how many months each river caused flooding. For the probability distribution of the maximum level of the river we can look to Extreme Value Theory which tells us that maximums are Gumbel distributed. How many times flooding occured will be modeled according to a Beta distribution which just tells us the probability of flooding to occur as a function of how many times flooding vs non-flooding occured.\nIt’s pretty reasonable to assume that the maximum level and number of floodings is going to be correlated. However, here we run into a problem: how should we model that probability distribution? Above we only specified the distributions for the individual variables, irrespective of the other one (i.e. the marginals). In reality we are dealing with a joint distribution of both of these together.\nCopulas to the rescue."
  },
  {
    "objectID": "blog/copulas - Copy.html#what-are-copulas-in-english",
    "href": "blog/copulas - Copy.html#what-are-copulas-in-english",
    "title": "An intuitive, visual guide to copulas",
    "section": "What are copulas in English?",
    "text": "What are copulas in English?\nCopulas allow us to decompose a joint probability distribution into their marginals (which by definition have no correlation) and a function which couples (hence the name) them together and thus allows us to specify the correlation seperately. The copula is that coupling function.\nBefore we dive into them, we must first learn how we can transform arbitrary random variables to uniform and back. All we will need is the excellent scipy.stats module and seaborn for plotting.\n\n%matplotlib inline\n\nimport seaborn as sns\nfrom scipy import stats"
  },
  {
    "objectID": "blog/copulas - Copy.html#transforming-random-variables",
    "href": "blog/copulas - Copy.html#transforming-random-variables",
    "title": "An intuitive, visual guide to copulas",
    "section": "Transforming random variables",
    "text": "Transforming random variables\nLet’s start by sampling uniformly distributed values between 0 and 1:\n\nx = stats.uniform(0, 1).rvs(10000)\nsns.distplot(x, kde=False, norm_hist=True);\n\n\n\n\nNext, we want to transform these samples so that instead of uniform they are now normally distributed. The transform that does this is the inverse of the cumulative density function (CDF) of the normal distribution (which we can get in scipy.stats with ppf):\n\nnorm = stats.distributions.norm()\nx_trans = norm.ppf(x)\nsns.distplot(x_trans);\n\n\n\n\nIf we plot both of them together we can get an intuition for what the inverse CDF looks like and how it works:\n\nh = sns.jointplot(x, x_trans, stat_func=None)\nh.set_axis_labels('original', 'transformed', fontsize=16);\n\n\n\n\nAs you can see, the inverse CDF stretches the outer regions of the uniform to yield a normal.\nWe can do this for arbitrary (univariate) probability distributions, like the Beta:\n\nbeta = stats.distributions.beta(a=10, b=3)\nx_trans = beta.ppf(x)\nh = sns.jointplot(x, x_trans, stat_func=None)\nh.set_axis_labels('orignal', 'transformed', fontsize=16);\n\n\n\n\nOr a Gumbel:\n\ngumbel = stats.distributions.gumbel_l()\nx_trans = gumbel.ppf(x)\nh = sns.jointplot(x, x_trans, stat_func=None)\nh.set_axis_labels('original', 'transformed', fontsize=16);\n\n\n\n\nIn order to do the opposite transformation from an arbitrary distribution to the uniform(0, 1) we just apply the inverse of the inverse CDF – the CDF:\n\nx_trans_trans = gumbel.cdf(x_trans)\nh = sns.jointplot(x_trans, x_trans_trans, stat_func=None)\nh.set_axis_labels('original', 'transformed', fontsize=16);\n\n\n\n\nOK, so we know how to transform from any distribution to uniform and back. In math-speak this is called the probability integral transform."
  },
  {
    "objectID": "blog/copulas - Copy.html#adding-correlation-with-gaussian-copulas",
    "href": "blog/copulas - Copy.html#adding-correlation-with-gaussian-copulas",
    "title": "An intuitive, visual guide to copulas",
    "section": "Adding correlation with Gaussian copulas",
    "text": "Adding correlation with Gaussian copulas\nHow does this help us with our problem of creating a custom joint probability distribution? We’re actually almost done already. We know how to convert anything uniformly distributed to an arbitrary probability distribution. So that means we need to generate uniformly distributed data with the correlations we want. How do we do that? We simulate from a multivariate Gaussian with the specific correlation structure, transform so that the marginals are uniform, and then transform the uniform marginals to whatever we like.\nCreate samples from a correlated multivariate normal:\n\nmvnorm = stats.multivariate_normal(mean=[0, 0], cov=[[1., 0.5], \n                                                     [0.5, 1.]])\n# Generate random samples from multivariate normal with correlation .5\nx = mvnorm.rvs(100000)\n\n\nh = sns.jointplot(x[:, 0], x[:, 1], kind='kde', stat_func=None);\nh.set_axis_labels('X1', 'X2', fontsize=16);\n\n\n\n\nNow use what we learned above to “uniformify” the marignals:\n\nnorm = stats.norm()\nx_unif = norm.cdf(x)\nh = sns.jointplot(x_unif[:, 0], x_unif[:, 1], kind='hex', stat_func=None)\nh.set_axis_labels('Y1', 'Y2', fontsize=16);\n\n\n\n\nThis joint plot above is usually how copulas are visualized.\nNow we just transform the marginals again to what we want (Gumbel and Beta):\n\nm1 = stats.gumbel_l()\nm2 = stats.beta(a=10, b=2)\n\nx1_trans = m1.ppf(x_unif[:, 0])\nx2_trans = m2.ppf(x_unif[:, 1])\n\nh = sns.jointplot(x1_trans, x2_trans, kind='kde', xlim=(-6, 2), ylim=(.6, 1.0), stat_func=None);\nh.set_axis_labels('Maximum river level', 'Probablity of flooding', fontsize=16);\n\n\n\n\nContrast that with the joint distribution without correlations:\n\nx1 = m1.rvs(10000)\nx2 = m2.rvs(10000)\n\nh = sns.jointplot(x1, x2, kind='kde', xlim=(-6, 2), ylim=(.6, 1.0), stat_func=None);\nh.set_axis_labels('Maximum river level', 'Probablity of flooding',  fontsize=16);\n\n\n\n\nSo there we go, by using the uniform distribution as our lingua franca we can easily induce correlations and flexibly construct complex probability distributions. This all directly extends to higher dimensional distributions as well."
  },
  {
    "objectID": "blog/copulas - Copy.html#more-complex-correlation-structures-and-the-financial-crisis",
    "href": "blog/copulas - Copy.html#more-complex-correlation-structures-and-the-financial-crisis",
    "title": "An intuitive, visual guide to copulas",
    "section": "More complex correlation structures and the Financial Crisis",
    "text": "More complex correlation structures and the Financial Crisis\nAbove we used a multivariate normal which gave rise to the Gaussian copula. However, we can use other, more complex copulas as well. For example, we might want to assume the correlation is non-symmetric which is useful in quant finance where correlations become very strong during market crashes and returns are very negative.\nIn fact, Gaussian copulas are said to have played a key role in the 2007-2008 Financial Crisis as tail-correlations were severely underestimated. If you’ve seen The Big Short, the default rates of individual mortgages (among other things) inside CDOs (see this scene from the movie as a refresher) are correlated – if one mortgage fails, the likelihood of another failing is increased. In the early 2000s, the banks only knew how to model the marginals of the default rates. This infamous paper by Li then suggested to use copulas to model the correlations between those marginals. Rating agencies relied on this model heavily, severly underestimating risk and giving false ratings. The rest, as they say, is history.\nRead this paper for an excellent description of Gaussian copulas and the Financial Crisis which argues that different copula choices would not have made a difference but instead the assumed correlation was way too low."
  },
  {
    "objectID": "blog/copulas - Copy.html#getting-back-to-the-math",
    "href": "blog/copulas - Copy.html#getting-back-to-the-math",
    "title": "An intuitive, visual guide to copulas",
    "section": "Getting back to the math",
    "text": "Getting back to the math\nMaybe now the statement “a copula is a multivariate distribution \\(C(U_1, U_2, ...., U_n)\\) such that marginalizing gives \\(U_i \\sim \\operatorname{\\sf Uniform}(0, 1)\\)” makes a bit more sense. It really is just a function with that property of uniform marginals. It’s really only useful though combined with another transform to get the marginals we want.\nWe can also better understand the mathematical description of the Gaussian copula (taken from Wikipedia):\n\nFor a given \\(R\\in[-1, 1]^{d\\times d}\\), the Gaussian copula with parameter matrix R can be written as \\(C_R^{\\text{Gauss}}(u) = \\Phi_R\\left(\\Phi^{-1}(u_1),\\dots, \\Phi^{-1}(u_d) \\right)\\) where \\(\\Phi^{-1}\\) is the inverse cumulative distribution function of a standard normal and \\(\\Phi_R\\) is the joint cumulative distribution function of a multivariate normal distribution with mean vector zero and covariance matrix equal to the correlation matrix R.\n\nJust note that in the code above we went the opposite way to create samples from that distribution. The Gaussian copula as expressed here takes uniform(0, 1) inputs, transforms them to be Gaussian, then applies the correlation and transforms them back to uniform."
  },
  {
    "objectID": "blog/copulas - Copy.html#support-me-on-patreon",
    "href": "blog/copulas - Copy.html#support-me-on-patreon",
    "title": "An intuitive, visual guide to copulas",
    "section": "Support me on Patreon",
    "text": "Support me on Patreon\nFinally, if you enjoyed this blog post, consider supporting me on Patreon which allows me to devote more time to writing new blog posts."
  },
  {
    "objectID": "blog/copulas - Copy.html#more-reading",
    "href": "blog/copulas - Copy.html#more-reading",
    "title": "An intuitive, visual guide to copulas",
    "section": "More reading",
    "text": "More reading\nThis post is intentionally light on math. You can find that elsewhere and will hopefully be less confused as you have a strong mental model to integrate things into. I found these links helpful:\n\nTensorflow Probability Bijection tutorial using copulas\nWikipedia article on copulas\nMatlab tutorial\nThe underlying NB of this post\n\nWe also haven’t addressed how we would actually fit a copula model. I leave that, as well as the PyMC3 implementation, as an exercise to the motivated reader ;)."
  },
  {
    "objectID": "blog/copulas - Copy.html#acknowledgements",
    "href": "blog/copulas - Copy.html#acknowledgements",
    "title": "An intuitive, visual guide to copulas",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Adrian Seyboldt, Jon Sedar, Colin Carroll, and Osvaldo Martin for comments on an earlier draft. Special thanks to Jonathan Ng for being a Patreon supporter."
  },
  {
    "objectID": "blog/GLM_hierarchical - Copy.html",
    "href": "blog/GLM_hierarchical - Copy.html",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "Thomas Wiecki & Danne Elbers 2020\n\nThe power of Bayesian modelling really clicked for me when I was first introduced to hierarchical modelling. In this blog post we will highlight the advantage of using hierarchical Bayesian modelling as opposed to non-hierarchical Bayesian modelling. This hierachical modelling is especially advantageous when multi-level data is used, making the most of all information available by its ‘shrinkage-effect’, which will be explained below.\nHaving multiple sets of measurements comes up all the time, in Psychology for example you test multiple subjects on the same task. You then might want to estimate a model that describes the behavior as a set of parameters relating to mental functioning. Often we are interested in individual differences of these parameters but also assume that subjects share similarities (being human and all). Software from our lab, HDDM, allows hierarchical Bayesian estimation of a widely used decision making model but we will use a more classical example of hierarchical linear regression here to predict radon levels in houses.\nThis is the 3rd blog post on the topic of Bayesian modeling in PyMC3, see here for the previous two:\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\nThis world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n\n\n\nGelman et al.’s (2007) radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all county’s of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to enter the house through the basement. Moreover, its concentration is thought to differ regionally due to different types of soil.\nHere we’ll investigate this difference and try to make predictions of radon levels in different countys and where in the house radon was measured. In this example we’ll look at Minnesota, a state that contains 85 county’s in which different measurements are taken, ranging from 2 till 80 measurements per county.\n\n\n\nradon\n\n\nFirst, we’ll load the data:\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm \nimport pandas as pd\n\ndata = pd.read_csv('radon.csv')\n\ncounty_names = data.county.unique()\ncounty_idx = data['county_code'].values\n\nThe relevant part of the data we will model looks as follows:\n\ndata[['county', 'log_radon', 'floor']].head()\n\n\n\n\n\n\n\n\ncounty\nlog_radon\nfloor\n\n\n\n\n0\nAITKIN\n0.832909\n1.0\n\n\n1\nAITKIN\n0.832909\n0.0\n\n\n2\nAITKIN\n1.098612\n0.0\n\n\n3\nAITKIN\n0.095310\n0.0\n\n\n4\nANOKA\n1.163151\n0.0\n\n\n\n\n\n\n\nAs you can see, we have multiple radon measurements (log-converted to be on the real line) in a county and whether the measurement has been taken in the basement (floor == 0) or on the first floor (floor == 1). Here we want to test the prediction that radon concentrations are higher in the basement.\n\n\n\n\n\nNow you might say: “That’s easy! I’ll just pool all my data and estimate one big regression to asses the influence of measurement across all counties”. In math-speak that model would be:\n\\[radon_{i, c} = \\alpha + \\beta*\\text{floor}_{i, c} + \\epsilon\\]\nWhere \\(i\\) represents the measurement, \\(c\\) the county and floor contains which floor the measurement was made. If you need a refresher on Linear Regressions in PyMC3, check out my previous blog post. Critically, we are only estimating one intercept and one slope for all measurements over all counties.\n\n\n\nBut what if we are interested whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then you might say “OK then, I’ll just estimate \\(n\\) (number of counties) different regresseions – one for each county”. In math-speak that model would be:\n\\[radon_{i, c} = \\alpha_{c} + \\beta_{c}*\\text{floor}_{i, c} + \\epsilon_c\\]\nNote that we added the subindex \\(c\\) so we are estimating \\(n\\) different \\(\\alpha\\)s and \\(\\beta\\)s – one for each county.\nThis is the extreme opposite model, where above we assumed all counties are exactly the same, here we are saying that they share no similarities whatsoever which ultimately is also unsatisifying.\n\n\n\nFortunately there is a middle ground to both of these extreme views. Specifically, we may assume that while \\(\\alpha\\)s and \\(\\beta\\)s are different for each county, the coefficients all come from a common group distribution:\n\\[\\alpha_{c} \\sim \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)\\] \\[\\beta_{c} \\sim \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta}^2)\\]\nWe thus assume the intercepts \\(\\alpha\\) and slopes \\(\\beta\\) to come from a normal distribution centered around their respective group mean \\(\\mu\\) with a certain standard deviation \\(\\sigma^2\\), the values (or rather posteriors) of which we also estimate. That’s why this is called multilevel or hierarchical modeling.\nHow do we estimate such a complex model with all these parameters you might ask? Well, that’s the beauty of Probabilistic Programming – we just formulate the model we want and press our Inference Button(TM).\nNote that the above is not a complete Bayesian model specification as we haven’t defined priors or hyperpriors (i.e. priors for the group distribution, \\(\\mu\\) and \\(\\sigma\\)). These will be used in the model implementation below but only distract here.\n\n\n\n\n\n\nTo really highlight the effect of the hierarchical linear regression we’ll first estimate the non-hierarchical Bayesian model from above (separate regressions). For each county a new estimate of the parameters is initiated. As we have no prior information on what the intercept or regressions could be we are placing a Normal distribution centered around 0 with a wide standard-deviation. We’ll assume the measurements are normally distributed with noise \\(\\epsilon\\) on which we place a Half-Cauchy distribution.\n\n# takes about 45 minutes\nindiv_traces = {}\nfor county_name in county_names:\n    # Select subset of data belonging to county\n    c_data = data.loc[data.county == county_name]\n    c_data = c_data.reset_index(drop=True)\n    \n    c_log_radon = c_data.log_radon\n    c_floor_measure = c_data.floor.values\n    \n    with pm.Model() as individual_model:\n        # Intercept prior\n        a = pm.Normal('alpha', mu=0, sigma=1)\n        # Slope prior\n        b = pm.Normal('beta', mu=0, sigma=1)\n    \n        # Model error prior\n        eps = pm.HalfCauchy('eps', beta=1)\n    \n        # Linear model\n        radon_est = a + b * c_floor_measure\n    \n        # Data likelihood\n        y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=c_log_radon)\n\n        # Inference button (TM)!\n        trace = pm.sample(progressbar=False)\n        \n    indiv_traces[county_name] = trace\n\n\n\n\n\nInstead of initiating the parameters separatly, the hierarchical model initiates group parameters that consider the county’s not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county’s \\(\\alpha\\) and \\(\\beta\\).\n\nwith pm.Model() as hierarchical_model:\n    # Hyperpriors\n    mu_a = pm.Normal('mu_alpha', mu=0., sigma=1)\n    sigma_a = pm.HalfCauchy('sigma_alpha', beta=1)\n    mu_b = pm.Normal('mu_beta', mu=0., sigma=1)\n    sigma_b = pm.HalfCauchy('sigma_beta', beta=1)\n    \n    # Intercept for each county, distributed around group mean mu_a\n    a = pm.Normal('alpha', mu=mu_a, sigma=sigma_a, shape=len(data.county.unique()))\n    # Intercept for each county, distributed around group mean mu_a\n    b = pm.Normal('beta', mu=mu_b, sigma=sigma_b, shape=len(data.county.unique()))\n    \n    # Model error\n    eps = pm.HalfCauchy('eps', beta=1)\n    \n    # Expected value\n    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n    \n    # Data likelihood\n    y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=data.log_radon)\n\n\nwith hierarchical_model:\n    hierarchical_trace = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [eps, beta, alpha, sigma_beta, mu_beta, sigma_alpha, mu_alpha]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 29 seconds.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 47 divergences after tuning. Increase `target_accept` or reparameterize.\nThe acceptance probability does not match the target. It is 0.7013813577935659, but should be close to 0.8. Try to increase the number of tuning steps.\nThe rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\nThe estimated number of effective samples is smaller than 200 for some parameters.\n\n\n\n    \n        \n      \n      100.00% [4000/4000 00:18&lt;00:00 Sampling 2 chains, 50 divergences]\n    \n    \n\n\n\npm.traceplot(hierarchical_trace);\n\n\n\n\nThe marginal posteriors in the left column are highly informative. mu_a tells us the group mean (log) radon levels. mu_b tells us that the slope is significantly negative (no mass above zero), meaning that radon concentrations are higher in the basement than first floor. We can also see by looking at the marginals for a that there is quite some differences in radon levels between counties; the different widths are related to how much measurements we have per county, the more, the higher our confidence in that parameter estimate.\n\nAfter writing this blog post I found out that the chains here (which look worse after I just re-ran them) are not properly converged, you can see that best for sigma_beta but also the warnings about “diverging samples” (which are also new in PyMC3). If you want to learn more about the problem and its solution, see my more recent blog post “Why hierarchical models are awesome, tricky, and Bayesian”.\n\n\n\n\n\n\nTo find out which of the models works better we can calculate the Root Mean Square Deviaton (RMSD). This posterior predictive check revolves around recreating the data based on the parameters found at different moments in the chain. The recreated or predicted values are subsequently compared to the real data points, the model that predicts data points closer to the original data is considered the better one. Thus, the lower the RMSD the better.\nWhen computing the RMSD (code not shown) we get the following result:\n\nindividual/non-hierarchical model: 0.13\nhierarchical model: 0.08\n\nAs can be seen above the hierarchical model performs a lot better than the non-hierarchical model in predicting the radon values. Following this, we’ll plot some examples of county’s showing the true radon values, the hierarchial predictions and the non-hierarchical predictions.\n\nselection = ['CASS', 'CROW WING', 'FREEBORN']\nfig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\naxis = axis.ravel()\nfor i, c in enumerate(selection):\n    c_data = data.loc[data.county == c]\n    c_data = c_data.reset_index(drop = True)\n    z = list(c_data['county_code'])[0]\n\n    xvals = np.linspace(-0.2, 1.2)\n    for a_val, b_val in zip(indiv_traces[c]['alpha'][::10], indiv_traces[c]['beta'][::10]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.05)\n    axis[i].plot(xvals, indiv_traces[c]['alpha'][::10].mean() + indiv_traces[c]['beta'][::10].mean() * xvals, \n                 'b', alpha=1, lw=2., label='individual')\n    for a_val, b_val in zip(hierarchical_trace['alpha'][::10][z], hierarchical_trace['beta'][::10][z]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.05)\n    axis[i].plot(xvals, hierarchical_trace['alpha'][::10][z].mean() + hierarchical_trace['beta'][::10][z].mean() * xvals, \n                 'g', alpha=1, lw=2., label='hierarchical')\n    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n                    alpha=1, color='k', marker='.', s=80, label='original data')\n    axis[i].set_xticks([0,1])\n    axis[i].set_xticklabels(['basement', 'first floor'])\n    axis[i].set_ylim(-1, 4)\n    axis[i].set_title(c)\n    if not i%3:\n        axis[i].legend()\n        axis[i].set_ylabel('log radon level')\n\n\n\n\nIn the above plot we have the data points in black of three selected counties. The thick lines represent the mean estimate of the regression line of the individual (blue) and hierarchical model (in green). The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are.\nWhen looking at the county ‘CASS’ we see that the non-hierarchical estimation has huge uncertainty about the radon levels of first floor measurements – that’s because we don’t have any measurements in this county. The hierarchical model, however, is able to apply what it learned about the relationship between floor and radon-levels from other counties to CASS and make sensible predictions even in the absence of measurements.\nWe can also see how the hierarchical model produces more robust estimates in ‘CROW WING’ and ‘FREEBORN’. In this regime of few data points the non-hierarchical model reacts more strongly to individual data points because that’s all it has to go on.\nHaving the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa."
  },
  {
    "objectID": "blog/GLM_hierarchical - Copy.html#the-data-set",
    "href": "blog/GLM_hierarchical - Copy.html#the-data-set",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "Gelman et al.’s (2007) radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all county’s of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to enter the house through the basement. Moreover, its concentration is thought to differ regionally due to different types of soil.\nHere we’ll investigate this difference and try to make predictions of radon levels in different countys and where in the house radon was measured. In this example we’ll look at Minnesota, a state that contains 85 county’s in which different measurements are taken, ranging from 2 till 80 measurements per county.\n\n\n\nradon\n\n\nFirst, we’ll load the data:\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm \nimport pandas as pd\n\ndata = pd.read_csv('radon.csv')\n\ncounty_names = data.county.unique()\ncounty_idx = data['county_code'].values\n\nThe relevant part of the data we will model looks as follows:\n\ndata[['county', 'log_radon', 'floor']].head()\n\n\n\n\n\n\n\n\ncounty\nlog_radon\nfloor\n\n\n\n\n0\nAITKIN\n0.832909\n1.0\n\n\n1\nAITKIN\n0.832909\n0.0\n\n\n2\nAITKIN\n1.098612\n0.0\n\n\n3\nAITKIN\n0.095310\n0.0\n\n\n4\nANOKA\n1.163151\n0.0\n\n\n\n\n\n\n\nAs you can see, we have multiple radon measurements (log-converted to be on the real line) in a county and whether the measurement has been taken in the basement (floor == 0) or on the first floor (floor == 1). Here we want to test the prediction that radon concentrations are higher in the basement."
  },
  {
    "objectID": "blog/GLM_hierarchical - Copy.html#the-models",
    "href": "blog/GLM_hierarchical - Copy.html#the-models",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "Now you might say: “That’s easy! I’ll just pool all my data and estimate one big regression to asses the influence of measurement across all counties”. In math-speak that model would be:\n\\[radon_{i, c} = \\alpha + \\beta*\\text{floor}_{i, c} + \\epsilon\\]\nWhere \\(i\\) represents the measurement, \\(c\\) the county and floor contains which floor the measurement was made. If you need a refresher on Linear Regressions in PyMC3, check out my previous blog post. Critically, we are only estimating one intercept and one slope for all measurements over all counties.\n\n\n\nBut what if we are interested whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then you might say “OK then, I’ll just estimate \\(n\\) (number of counties) different regresseions – one for each county”. In math-speak that model would be:\n\\[radon_{i, c} = \\alpha_{c} + \\beta_{c}*\\text{floor}_{i, c} + \\epsilon_c\\]\nNote that we added the subindex \\(c\\) so we are estimating \\(n\\) different \\(\\alpha\\)s and \\(\\beta\\)s – one for each county.\nThis is the extreme opposite model, where above we assumed all counties are exactly the same, here we are saying that they share no similarities whatsoever which ultimately is also unsatisifying.\n\n\n\nFortunately there is a middle ground to both of these extreme views. Specifically, we may assume that while \\(\\alpha\\)s and \\(\\beta\\)s are different for each county, the coefficients all come from a common group distribution:\n\\[\\alpha_{c} \\sim \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)\\] \\[\\beta_{c} \\sim \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta}^2)\\]\nWe thus assume the intercepts \\(\\alpha\\) and slopes \\(\\beta\\) to come from a normal distribution centered around their respective group mean \\(\\mu\\) with a certain standard deviation \\(\\sigma^2\\), the values (or rather posteriors) of which we also estimate. That’s why this is called multilevel or hierarchical modeling.\nHow do we estimate such a complex model with all these parameters you might ask? Well, that’s the beauty of Probabilistic Programming – we just formulate the model we want and press our Inference Button(TM).\nNote that the above is not a complete Bayesian model specification as we haven’t defined priors or hyperpriors (i.e. priors for the group distribution, \\(\\mu\\) and \\(\\sigma\\)). These will be used in the model implementation below but only distract here."
  },
  {
    "objectID": "blog/GLM_hierarchical - Copy.html#probabilistic-programming",
    "href": "blog/GLM_hierarchical - Copy.html#probabilistic-programming",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "To really highlight the effect of the hierarchical linear regression we’ll first estimate the non-hierarchical Bayesian model from above (separate regressions). For each county a new estimate of the parameters is initiated. As we have no prior information on what the intercept or regressions could be we are placing a Normal distribution centered around 0 with a wide standard-deviation. We’ll assume the measurements are normally distributed with noise \\(\\epsilon\\) on which we place a Half-Cauchy distribution.\n\n# takes about 45 minutes\nindiv_traces = {}\nfor county_name in county_names:\n    # Select subset of data belonging to county\n    c_data = data.loc[data.county == county_name]\n    c_data = c_data.reset_index(drop=True)\n    \n    c_log_radon = c_data.log_radon\n    c_floor_measure = c_data.floor.values\n    \n    with pm.Model() as individual_model:\n        # Intercept prior\n        a = pm.Normal('alpha', mu=0, sigma=1)\n        # Slope prior\n        b = pm.Normal('beta', mu=0, sigma=1)\n    \n        # Model error prior\n        eps = pm.HalfCauchy('eps', beta=1)\n    \n        # Linear model\n        radon_est = a + b * c_floor_measure\n    \n        # Data likelihood\n        y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=c_log_radon)\n\n        # Inference button (TM)!\n        trace = pm.sample(progressbar=False)\n        \n    indiv_traces[county_name] = trace"
  },
  {
    "objectID": "blog/GLM_hierarchical - Copy.html#hierarchical-model",
    "href": "blog/GLM_hierarchical - Copy.html#hierarchical-model",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "Instead of initiating the parameters separatly, the hierarchical model initiates group parameters that consider the county’s not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county’s \\(\\alpha\\) and \\(\\beta\\).\n\nwith pm.Model() as hierarchical_model:\n    # Hyperpriors\n    mu_a = pm.Normal('mu_alpha', mu=0., sigma=1)\n    sigma_a = pm.HalfCauchy('sigma_alpha', beta=1)\n    mu_b = pm.Normal('mu_beta', mu=0., sigma=1)\n    sigma_b = pm.HalfCauchy('sigma_beta', beta=1)\n    \n    # Intercept for each county, distributed around group mean mu_a\n    a = pm.Normal('alpha', mu=mu_a, sigma=sigma_a, shape=len(data.county.unique()))\n    # Intercept for each county, distributed around group mean mu_a\n    b = pm.Normal('beta', mu=mu_b, sigma=sigma_b, shape=len(data.county.unique()))\n    \n    # Model error\n    eps = pm.HalfCauchy('eps', beta=1)\n    \n    # Expected value\n    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n    \n    # Data likelihood\n    y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=data.log_radon)\n\n\nwith hierarchical_model:\n    hierarchical_trace = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [eps, beta, alpha, sigma_beta, mu_beta, sigma_alpha, mu_alpha]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 29 seconds.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 47 divergences after tuning. Increase `target_accept` or reparameterize.\nThe acceptance probability does not match the target. It is 0.7013813577935659, but should be close to 0.8. Try to increase the number of tuning steps.\nThe rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\nThe estimated number of effective samples is smaller than 200 for some parameters.\n\n\n\n    \n        \n      \n      100.00% [4000/4000 00:18&lt;00:00 Sampling 2 chains, 50 divergences]\n    \n    \n\n\n\npm.traceplot(hierarchical_trace);\n\n\n\n\nThe marginal posteriors in the left column are highly informative. mu_a tells us the group mean (log) radon levels. mu_b tells us that the slope is significantly negative (no mass above zero), meaning that radon concentrations are higher in the basement than first floor. We can also see by looking at the marginals for a that there is quite some differences in radon levels between counties; the different widths are related to how much measurements we have per county, the more, the higher our confidence in that parameter estimate.\n\nAfter writing this blog post I found out that the chains here (which look worse after I just re-ran them) are not properly converged, you can see that best for sigma_beta but also the warnings about “diverging samples” (which are also new in PyMC3). If you want to learn more about the problem and its solution, see my more recent blog post “Why hierarchical models are awesome, tricky, and Bayesian”."
  },
  {
    "objectID": "blog/GLM_hierarchical - Copy.html#posterior-predictive-check",
    "href": "blog/GLM_hierarchical - Copy.html#posterior-predictive-check",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "",
    "text": "To find out which of the models works better we can calculate the Root Mean Square Deviaton (RMSD). This posterior predictive check revolves around recreating the data based on the parameters found at different moments in the chain. The recreated or predicted values are subsequently compared to the real data points, the model that predicts data points closer to the original data is considered the better one. Thus, the lower the RMSD the better.\nWhen computing the RMSD (code not shown) we get the following result:\n\nindividual/non-hierarchical model: 0.13\nhierarchical model: 0.08\n\nAs can be seen above the hierarchical model performs a lot better than the non-hierarchical model in predicting the radon values. Following this, we’ll plot some examples of county’s showing the true radon values, the hierarchial predictions and the non-hierarchical predictions.\n\nselection = ['CASS', 'CROW WING', 'FREEBORN']\nfig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\naxis = axis.ravel()\nfor i, c in enumerate(selection):\n    c_data = data.loc[data.county == c]\n    c_data = c_data.reset_index(drop = True)\n    z = list(c_data['county_code'])[0]\n\n    xvals = np.linspace(-0.2, 1.2)\n    for a_val, b_val in zip(indiv_traces[c]['alpha'][::10], indiv_traces[c]['beta'][::10]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.05)\n    axis[i].plot(xvals, indiv_traces[c]['alpha'][::10].mean() + indiv_traces[c]['beta'][::10].mean() * xvals, \n                 'b', alpha=1, lw=2., label='individual')\n    for a_val, b_val in zip(hierarchical_trace['alpha'][::10][z], hierarchical_trace['beta'][::10][z]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.05)\n    axis[i].plot(xvals, hierarchical_trace['alpha'][::10][z].mean() + hierarchical_trace['beta'][::10][z].mean() * xvals, \n                 'g', alpha=1, lw=2., label='hierarchical')\n    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n                    alpha=1, color='k', marker='.', s=80, label='original data')\n    axis[i].set_xticks([0,1])\n    axis[i].set_xticklabels(['basement', 'first floor'])\n    axis[i].set_ylim(-1, 4)\n    axis[i].set_title(c)\n    if not i%3:\n        axis[i].legend()\n        axis[i].set_ylabel('log radon level')\n\n\n\n\nIn the above plot we have the data points in black of three selected counties. The thick lines represent the mean estimate of the regression line of the individual (blue) and hierarchical model (in green). The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are.\nWhen looking at the county ‘CASS’ we see that the non-hierarchical estimation has huge uncertainty about the radon levels of first floor measurements – that’s because we don’t have any measurements in this county. The hierarchical model, however, is able to apply what it learned about the relationship between floor and radon-levels from other counties to CASS and make sensible predictions even in the absence of measurements.\nWe can also see how the hierarchical model produces more robust estimates in ‘CROW WING’ and ‘FREEBORN’. In this regime of few data points the non-hierarchical model reacts more strongly to individual data points because that’s all it has to go on.\nHaving the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa."
  },
  {
    "objectID": "blog/GLM_hierarchical - Copy.html#references",
    "href": "blog/GLM_hierarchical - Copy.html#references",
    "title": "The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3",
    "section": "References",
    "text": "References\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\nThis world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n\nChris Fonnesbeck repo containing a more extensive analysis\nShrinkage in multi-level hierarchical models by John Kruschke\nGelman, A.; Carlin; Stern; and Rubin, D., 2007, “Replication data for: Bayesian Data Analysis, Second Edition”,\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press.\nGelman, A. (2006). Multilevel (Hierarchical) modeling: what it can and cannot do. Technometrics, 48(3), 432–435."
  },
  {
    "objectID": "blog/random_walk_deep_net.html",
    "href": "blog/random_walk_deep_net.html",
    "title": "Random-Walk Bayesian Deep Networks: Dealing with Non-Stationary Data",
    "section": "",
    "text": "Download the NB\nMost problems solved by Deep Learning are stationary. A cat is always a cat. The rules of Go have remained stable for 2,500 years, and will likely stay that way. However, what if the world around you is changing? This is common, for example when applying Machine Learning in Quantitative Finance. Markets are constantly evolving so features that are predictive in some time-period might lose their edge, while other patterns emerge. Usually, quants would just retrain their classifiers every once in a while. This approach of just re-estimating the same model on more recent data is very common. I find that to be a pretty unsatisfying way of modeling, as there are certain shortfalls:\nCertainly there is something to be learned even from past data, we just need to instill our models with a sense of time and recency.\nEnter random-walk processes. Ever since I learned about them in the stochastic volatility model they have become one of my favorite modeling tricks. Basically, it allows you to turn every static model into a time-sensitive one.\nYou can read more about the details of a random-walk priors here, but the central idea is that, in any time-series model, rather than assuming a parameter to be constant over time, we allow it to change gradually, following a random walk. For example, take a logistic regression:\n\\[\nY_i = f(\\beta X_i)\n\\]\nWhere \\(f\\) is the logistic function and \\(\\beta\\) is our learnable parameter. If we assume that our data is not iid and that \\(\\beta\\) is changing over time. We thus need a different \\(\\beta\\) for every \\(i\\):\n\\[\nY_i = f(\\beta_i X_i)\n\\]\nOf course, this will just overfit, so we need to constrain our \\(\\beta_i\\) somehow. We will assume that while \\(\\beta_i\\) is changing over time, it will do so rather gradually by placing a random-walk prior on it:\n\\[\n\\beta_t \\sim \\mathcal{N}(\\beta_{t-1}, s^2)\n\\]\nSo \\(\\beta_t\\) is allowed to only deviate a little bit (determined by the step-width \\(s\\)) form its previous value \\(\\beta_{t-1}\\). \\(s\\) can be thought of as a stability parameter – how fast is the world around you changing.\nLet’s first generate some toy data and then implement this model in PyMC3. We will then use this same trick in a Neural Network with hidden layers.\nIf you would like a more complete introduction to Bayesian Deep Learning, see my recent ODSC London talk. This blog post takes things one step further so definitely read further below.\n%matplotlib inline\nimport pymc3 as pm\nimport theano.tensor as T\nimport theano\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\n\n\nimport warnings\nfrom scipy import VisibleDeprecationWarning\nwarnings.filterwarnings(\"ignore\", category=VisibleDeprecationWarning) \n\nsns.set_context('notebook')\nFirst, lets generate some toy data – a simple binary classification problem that’s linearly separable. To introduce the non-stationarity, we will rotate this data along the center across time. Safely skip over the next few code cells.\nX, Y = sklearn.datasets.make_blobs(n_samples=1000, centers=2, random_state=1)\nX = scale(X)\ncolors = Y.astype(str)\ncolors[Y == 0] = 'r'\ncolors[Y == 1] = 'b'\n\ninterval = 20\nsubsample = X.shape[0] // interval\nchunk = np.arange(0, X.shape[0]+1, subsample)\ndegs = np.linspace(0, 360, len(chunk))\n\nsep_lines = []\n\nfor ii, (i, j, deg) in enumerate(list(zip(np.roll(chunk, 1), chunk, degs))[1:]):\n    theta = np.radians(deg)\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.matrix([[c, -s], [s, c]])\n\n    X[i:j, :] = X[i:j, :].dot(R)\nimport base64\nfrom tempfile import NamedTemporaryFile\n\nVIDEO_TAG = \"\"\"&lt;video controls&gt;\n &lt;source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\"&gt;\n Your browser does not support the video tag.\n&lt;/video&gt;\"\"\"\n\n\ndef anim_to_html(anim):\n    if not hasattr(anim, '_encoded_video'):\n        anim.save(\"test.mp4\", fps=20, extra_args=['-vcodec', 'libx264'])\n\n        video = open(\"test.mp4\",\"rb\").read()\n\n    anim._encoded_video = base64.b64encode(video).decode('utf-8')\n    return VIDEO_TAG.format(anim._encoded_video)\n\nfrom IPython.display import HTML\n\ndef display_animation(anim):\n    plt.close(anim._fig)\n    return HTML(anim_to_html(anim))\nfrom matplotlib import animation\n\n# First set up the figure, the axis, and the plot element we want to animate\nfig, ax = plt.subplots()\nims = [] #l, = plt.plot([], [], 'r-')\nfor i in np.arange(0, len(X), 10):\n    ims.append([(ax.scatter(X[:i, 0], X[:i, 1], color=colors[:i]))])\n\nax.set(xlabel='X1', ylabel='X2')\n# call the animator.  blit=True means only re-draw the parts that have changed.\nanim = animation.ArtistAnimation(fig, ims,\n                                 interval=500, \n                                 blit=True);\n\ndisplay_animation(anim)\n\n\n \n Your browser does not support the video tag.\nThe last frame of the video, where all data is plotted is what a classifier would see that has no sense of time. Thus, the problem we set up is impossible to solve when ignoring time, but trivial once you do.\nHow would we classically solve this? You could just train a different classifier on each subset. But as I wrote above, you need to get the frequency right and you use less data overall."
  },
  {
    "objectID": "blog/random_walk_deep_net.html#random-walk-logistic-regression-in-pymc3",
    "href": "blog/random_walk_deep_net.html#random-walk-logistic-regression-in-pymc3",
    "title": "Random-Walk Bayesian Deep Networks: Dealing with Non-Stationary Data",
    "section": "Random-Walk Logistic Regression in PyMC3",
    "text": "Random-Walk Logistic Regression in PyMC3\n\nfrom pymc3 import HalfNormal, GaussianRandomWalk, Bernoulli\nfrom pymc3.math import sigmoid\nimport theano.tensor as tt\n\n\nX_shared = theano.shared(X)\nY_shared = theano.shared(Y)\n\nn_dim = X.shape[1] # 2\n\nwith pm.Model() as random_walk_perceptron:\n    step_size = pm.HalfNormal('step_size', sd=np.ones(n_dim), \n                              shape=n_dim)\n    \n    # This is the central trick, PyMC3 already comes with this distribution\n    w = pm.GaussianRandomWalk('w', sd=step_size, \n                              shape=(interval, 2))\n    \n    weights = tt.repeat(w, X_shared.shape[0] // interval, axis=0)\n    \n    class_prob = sigmoid(tt.batched_dot(X_shared, weights))\n    \n    # Binary classification -&gt; Bernoulli likelihood\n    pm.Bernoulli('out', class_prob, observed=Y_shared)\n\nOK, if you understand the stochastic volatility model, the first two lines should look fairly familiar. We are creating 2 random-walk processes. As allowing the weights to change on every new data point is overkill, we subsample. The repeat turns the vector [t, t+1, t+2] into [t, t, t, t+1, t+1, ...] so that it matches the number of data points.\nNext, we would usually just apply a single dot-product but here we have many weights we’re applying to the input data, so we need to call dot in a loop. That is what tt.batched_dot does. In the end, we just get probabilities (predicitions) for our Bernoulli likelihood.\nOn to the inference. In PyMC3 we recently improved NUTS in many different places. One of those is automatic initialization. If you just call pm.sample(n_iter), we will first run ADVI to estimate the diagional mass matrix and find a starting point. This usually makes NUTS run quite robustly.\n\nwith random_walk_perceptron:\n    trace_perceptron = pm.sample(2000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using advi...\nAverage ELBO = -90.867: 100%|██████████| 200000/200000 [01:13&lt;00:00, 2739.70it/s]\nFinished [100%]: Average ELBO = -90.869\n100%|██████████| 2000/2000 [00:39&lt;00:00, 50.58it/s]\n\n\nLet’s look at the learned weights over time:\n\nplt.plot(trace_perceptron['w'][:, :, 0].T, alpha=.05, color='r');\nplt.plot(trace_perceptron['w'][:, :, 1].T, alpha=.05, color='b');\nplt.xlabel('time'); plt.ylabel('weights'); plt.title('Optimal weights change over time'); sns.despine();\n\n\n\n\nAs you can see, the weights are slowly changing over time. What does the learned hyperplane look like? In the plot below, the points are still the training data but the background color codes the class probability learned by the model.\n\ngrid = np.mgrid[-3:3:100j,-3:3:100j]\ngrid_2d = grid.reshape(2, -1).T\ngrid_2d = np.tile(grid_2d, (interval, 1))\ndummy_out = np.ones(grid_2d.shape[0], dtype=np.int8)\n\nX_shared.set_value(grid_2d)\nY_shared.set_value(dummy_out)\n\n# Creater posterior predictive samples\nppc = pm.sample_ppc(trace_perceptron, model=random_walk_perceptron, samples=500)\n\ndef create_surface(X, Y, grid, ppc, fig=None, ax=None):\n    artists = []\n    cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n    contour = ax.contourf(*grid, ppc, cmap=cmap)\n    artists.extend(contour.collections)\n    artists.append(ax.scatter(X[Y==0, 0], X[Y==0, 1], color='b'))\n    artists.append(ax.scatter(X[Y==1, 0], X[Y==1, 1], color='r'))\n    _ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X1', ylabel='X2');\n    return artists\n\nfig, ax = plt.subplots()\nchunk = np.arange(0, X.shape[0]+1, subsample)\nchunk_grid = np.arange(0, grid_2d.shape[0]+1, 10000)\naxs = []\nfor (i, j), (i_grid, j_grid) in zip((list(zip(np.roll(chunk, 1), chunk))[1:]), (list(zip(np.roll(chunk_grid, 1), chunk_grid))[1:])):\n    a = create_surface(X[i:j], Y[i:j], grid, ppc['out'][:, i_grid:j_grid].mean(axis=0).reshape(100, 100), fig=fig, ax=ax)\n    axs.append(a)\n    \nanim2 = animation.ArtistAnimation(fig, axs,\n                                 interval=1000);\ndisplay_animation(anim2)\n\n100%|██████████| 500/500 [00:23&lt;00:00, 24.47it/s]\n\n\n\n \n Your browser does not support the video tag.\n\n\n\nNice, we can see that the random-walk logistic regression adapts its weights to perfectly separate the two point clouds."
  },
  {
    "objectID": "blog/random_walk_deep_net.html#random-walk-neural-network",
    "href": "blog/random_walk_deep_net.html#random-walk-neural-network",
    "title": "Random-Walk Bayesian Deep Networks: Dealing with Non-Stationary Data",
    "section": "Random-Walk Neural Network",
    "text": "Random-Walk Neural Network\nIn the previous example, we had a very simple linearly classifiable problem. Can we extend this same idea to non-linear problems and build a Bayesian Neural Network with weights adapting over time?\nIf you haven’t, I recommend you read my original post on Bayesian Deep Learning where I more thoroughly explain how a Neural Network can be implemented and fit in PyMC3.\nLets generate some toy data that is not linearly separable and again rotate it around its center.\n\nfrom sklearn.datasets import make_moons\nX, Y = make_moons(noise=0.2, random_state=0, n_samples=5000)\nX = scale(X)\n\ncolors = Y.astype(str)\ncolors[Y == 0] = 'r'\ncolors[Y == 1] = 'b'\n\ninterval = 20\nsubsample = X.shape[0] // interval\nchunk = np.arange(0, X.shape[0]+1, subsample)\ndegs = np.linspace(0, 360, len(chunk))\n\nsep_lines = []\n\nfor ii, (i, j, deg) in enumerate(list(zip(np.roll(chunk, 1), chunk, degs))[1:]):\n    theta = np.radians(deg)\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.matrix([[c, -s], [s, c]])\n\n    X[i:j, :] = X[i:j, :].dot(R)\n\n\nfig, ax = plt.subplots()\nims = []\nfor i in np.arange(0, len(X), 10):\n    ims.append((ax.scatter(X[:i, 0], X[:i, 1], color=colors[:i]),))\n\nax.set(xlabel='X1', ylabel='X2')\nanim = animation.ArtistAnimation(fig, ims,\n                                 interval=500, \n                                 blit=True);\n\ndisplay_animation(anim)\n\n\n \n Your browser does not support the video tag.\n\n\n\nLooks a bit like Ying and Yang, who knew we’d be creating art in the process.\nOn to the model. Rather than have all the weights in the network follow random-walks, we will just have the first hidden layer change its weights. The idea is that the higher layers learn stable higher-order representations while the first layer is transforming the raw data so that it appears stationary to the higher layers. We can of course also place random-walk priors on all weights, or only on those of higher layers, whatever assumptions you want to build into the model.\n\nnp.random.seed(123)\n\nann_input = theano.shared(X)\nann_output = theano.shared(Y)\n\nn_hidden = [2, 5]\n\n# Initialize random weights between each layer\ninit_1 = np.random.randn(X.shape[1], n_hidden[0]).astype(theano.config.floatX)\ninit_2 = np.random.randn(n_hidden[0], n_hidden[1]).astype(theano.config.floatX)\ninit_out = np.random.randn(n_hidden[1]).astype(theano.config.floatX)\n    \nwith pm.Model() as neural_network:\n    # Weights from input to hidden layer\n    step_size = pm.HalfNormal('step_size', sd=np.ones(n_hidden[0]), \n                              shape=n_hidden[0])\n    \n    weights_in_1 = pm.GaussianRandomWalk('w1', sd=step_size, \n                                         shape=(interval, X.shape[1], n_hidden[0]),\n                                         testval=np.tile(init_1, (interval, 1, 1))\n                                        )\n    \n    weights_in_1_rep = tt.repeat(weights_in_1, \n                                 ann_input.shape[0] // interval, axis=0)\n    \n    weights_1_2 = pm.Normal('w2', mu=0, sd=1., \n                            shape=(1, n_hidden[0], n_hidden[1]),\n                            testval=init_2)\n    \n    weights_1_2_rep = tt.repeat(weights_1_2, \n                                ann_input.shape[0], axis=0)\n    \n    weights_2_out = pm.Normal('w3', mu=0, sd=1.,\n                              shape=(1, n_hidden[1]),\n                              testval=init_out)\n    \n    weights_2_out_rep = tt.repeat(weights_2_out, \n                                  ann_input.shape[0], axis=0)\n      \n\n    # Build neural-network using tanh activation function\n    act_1 = tt.tanh(tt.batched_dot(ann_input, \n                         weights_in_1_rep))\n    act_2 = tt.tanh(tt.batched_dot(act_1, \n                         weights_1_2_rep))\n    act_out = tt.nnet.sigmoid(tt.batched_dot(act_2, \n                                             weights_2_out_rep))\n        \n    # Binary classification -&gt; Bernoulli likelihood\n    out = pm.Bernoulli('out', \n                       act_out,\n                       observed=ann_output)\n\nHopefully that’s not too incomprehensible. It is basically applying the principles from the random-walk logistic regression but adding another hidden layer.\nI also want to take the opportunity to look at what the Bayesian approach to Deep Learning offers. Usually, we fit these models using point-estimates like the MLE or the MAP. Let’s see how well that works on a structually more complex model like this one:\n\nimport scipy.optimize\nwith neural_network:\n    map_est = pm.find_MAP(fmin=scipy.optimize.fmin_l_bfgs_b)\n\n\nplt.plot(map_est['w1'].reshape(20, 4));\n\n\n\n\nSome of the weights are changing, maybe it worked? How well does it fit the training data:\n\nppc = pm.sample_ppc([map_est], model=neural_network, samples=1)\nprint('Accuracy on train data = {:.2f}%'.format((ppc['out'] == Y).mean() * 100))\n\n100%|██████████| 1/1 [00:00&lt;00:00,  6.32it/s]\n\n\nAccuracy on train data = 76.64%\n\n\nNow on to estimating the full posterior, as a proper Bayesian would:\n\nwith neural_network:\n    trace = pm.sample(1000, tune=200)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using advi...\nAverage ELBO = -538.86: 100%|██████████| 200000/200000 [13:06&lt;00:00, 254.43it/s]\nFinished [100%]: Average ELBO = -538.69\n100%|██████████| 1000/1000 [1:22:05&lt;00:00,  4.97s/it]\n\n\n\nplt.plot(trace['w1'][200:, :, 0, 0].T, alpha=.05, color='r');\nplt.plot(trace['w1'][200:, :, 0, 1].T, alpha=.05, color='b');\nplt.plot(trace['w1'][200:, :, 1, 0].T, alpha=.05, color='g');\nplt.plot(trace['w1'][200:, :, 1, 1].T, alpha=.05, color='c');\n\nplt.xlabel('time'); plt.ylabel('weights'); plt.title('Optimal weights change over time'); sns.despine();\n\n\n\n\nThat already looks quite different. What about the accuracy:\n\nppc = pm.sample_ppc(trace, model=neural_network, samples=100)\nprint('Accuracy on train data = {:.2f}%'.format(((ppc['out'].mean(axis=0) &gt; .5) == Y).mean() * 100))\n\n100%|██████████| 100/100 [00:00&lt;00:00, 112.04it/s]\n\n\nAccuracy on train data = 96.72%\n\n\nI think this is worth highlighting. The point-estimate did not do well at all, but by estimating the whole posterior we were able to model the data much more accurately. I’m not quite sure why that is the case. It’s possible that we either did not find the true MAP because the optimizer can’t deal with the correlations in the posterior as well as NUTS can, or the MAP is just not a good point. See my other blog post on hierarchical models as for why the MAP is a terrible choice for some models.\nOn to the fireworks. What does this actually look like:\n\ngrid = np.mgrid[-3:3:100j,-3:3:100j]\ngrid_2d = grid.reshape(2, -1).T\ngrid_2d = np.tile(grid_2d, (interval, 1))\ndummy_out = np.ones(grid_2d.shape[0], dtype=np.int8)\n\nann_input.set_value(grid_2d)\nann_output.set_value(dummy_out)\n\n# Creater posterior predictive samples\nppc = pm.sample_ppc(trace, model=neural_network, samples=500)\n\nfig, ax = plt.subplots()\nchunk = np.arange(0, X.shape[0]+1, subsample)\nchunk_grid = np.arange(0, grid_2d.shape[0]+1, 10000)\naxs = []\nfor (i, j), (i_grid, j_grid) in zip((list(zip(np.roll(chunk, 1), chunk))[1:]), (list(zip(np.roll(chunk_grid, 1), chunk_grid))[1:])):\n    a = create_surface(X[i:j], Y[i:j], grid, ppc['out'][:, i_grid:j_grid].mean(axis=0).reshape(100, 100), fig=fig, ax=ax)\n    axs.append(a)\n    \nanim2 = animation.ArtistAnimation(fig, axs,\n                                  interval=1000);\ndisplay_animation(anim2)\n\n100%|██████████| 500/500 [00:58&lt;00:00,  7.82it/s]\n\n\n\n \n Your browser does not support the video tag.\n\n\n\nHoly shit! I can’t believe that actually worked. Just for fun, let’s also make use of the fact that we have the full posterior and plot our uncertainty of our prediction (the background now encodes posterior standard-deviation where red means high uncertainty).\n\nfig, ax = plt.subplots()\nchunk = np.arange(0, X.shape[0]+1, subsample)\nchunk_grid = np.arange(0, grid_2d.shape[0]+1, 10000)\naxs = []\nfor (i, j), (i_grid, j_grid) in zip((list(zip(np.roll(chunk, 1), chunk))[1:]), (list(zip(np.roll(chunk_grid, 1), chunk_grid))[1:])):\n    a = create_surface(X[i:j], Y[i:j], grid, ppc['out'][:, i_grid:j_grid].std(axis=0).reshape(100, 100), \n                       fig=fig, ax=ax)\n    axs.append(a)\n\nanim2 = animation.ArtistAnimation(fig, axs,\n                                  interval=1000);\ndisplay_animation(anim2)\n\n\n \n Your browser does not support the video tag."
  },
  {
    "objectID": "blog/random_walk_deep_net.html#conclusions",
    "href": "blog/random_walk_deep_net.html#conclusions",
    "title": "Random-Walk Bayesian Deep Networks: Dealing with Non-Stationary Data",
    "section": "Conclusions",
    "text": "Conclusions\nIn this blog post I explored the possibility of extending Neural Networks in new ways (to my knowledge), enabled by expressing them in a Probabilistic Programming framework. Using a classic point-estimate did not provide a good fit for the data, only full posterior inference using MCMC allowed us to fit this model adequately. What is quite nice, is that we did not have to do anything special for the inference in PyMC3, just calling pymc3.sample() gave stable results on this complex model.\nInitially I built the model allowing all parameters to change, but realizing that we can selectively choose which layers to change felt like a profound insight. If you expect the raw data to change, but the higher-level representations to remain stable, as was the case here, we allow the bottom hidden layers to change. If we instead imagine e.g. handwriting recognition, where your handwriting might change over time, we would expect lower level features (lines, curves) to remain stable but allow changes in how we combine them. Finally, if the world remains stable but the labels change, we would place a random-walk process on the output layer. Of course, if you don’t know, you can have every layer change its weights over time and give each one a separate step-size parameter which would allow the model to figure out which layers change (high step-size), and which remain stable (low step-size).\nIn terms of quantatitative finance, this type of model allows us to train on much larger data sets ranging back a long time. A lot of that data is still useful to build up stable hidden representations, even if for predicition you still want your model to predict using its most up-to-date state of the world. No need to define a window-length or discard valuable training data.\n\n%load_ext watermark\n%watermark -v -m -p numpy,scipy,sklearn,theano,pymc3,matplotlib\n\nThe watermark extension is already loaded. To reload it, use:\n  %reload_ext watermark\nCPython 3.6.0\nIPython 5.1.0\n\nnumpy 1.11.3\nscipy 0.18.1\nsklearn 0.18.1\ntheano 0.9.0beta1.dev-9f1aaacb6e884ebcff9e249f19848db8aa6cb1b2\npymc3 3.0\nmatplotlib 2.0.0\n\ncompiler   : GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)\nsystem     : Darwin\nrelease    : 16.4.0\nmachine    : x86_64\nprocessor  : i386\nCPU cores  : 4\ninterpreter: 64bit"
  }
]